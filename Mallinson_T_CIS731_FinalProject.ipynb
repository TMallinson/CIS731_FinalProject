{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "Mallinson_T_CIS731_FinalProject.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRgNvo1jJ5d3",
        "colab_type": "text"
      },
      "source": [
        "<h1 style=\"text-align: center;\"><span style=\"color: #000000;\"> CIS 731 Final Project: Sentiment Analysis Using Pyspark on Fan's Tweets Following the NFL Draft\n",
        "</span></h1>\n",
        "<h3 style=\"text-align: center;\"><span style=\"color: #000000;\"> Created by: Thomas Mallinson\n",
        "</span></h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzzWLzwtJ5d4",
        "colab_type": "text"
      },
      "source": [
        "![combo.png](attachment:combo.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84QOIcf4J5d4",
        "colab_type": "text"
      },
      "source": [
        "# 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMGCR0kIJ5d5",
        "colab_type": "text"
      },
      "source": [
        "### 1.1. Procedure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmGIvaw5J5d5",
        "colab_type": "text"
      },
      "source": [
        "This project will descriptive analytics and content analytics on keywords pertaining to Tweets regarding the 2020 NFL Draft. For the first time ever, the draft was held virtually due to the COVID-19 Pandemic. I want to see how fans reacted to the new structure. I will scrape tweets using Tweepy and collect them using the cloud server PythonAnywhere. Tweet data will be analyzed by applying descriptive, content, and network analytics techniques using Python and PySpark. I will use a Naive Bayes classification algorithm to classify the text and sentiment analysis to record the feeling towards the results of the draft."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nej4kDgiJ5d5",
        "colab_type": "text"
      },
      "source": [
        "### 1.2. Procedure Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGoJagQKJ5d6",
        "colab_type": "text"
      },
      "source": [
        "**Cloud Server/Cloud Computing:** A cloud server is a logical server that is built, hosted and delivered through a cloud computing platform over the Internet. Cloud servers possess and exhibit similar capabilities and functionality to a typical server but are accessed remotely from a cloud service provider. PythonAnywhere is an online integrated development environment and web hosting service based on the Python programming language.\n",
        "\n",
        "**PySpark:** Apache Spark is an analytics engine and parallel computation framework with Scala, Python, Java, and R interfaces. Spark can load data directly from disk, memory and other data storage technologies such as Amazon S3, Hadoop Distributed File System (HDFS), Cassandra and others. PySpark is collaboration of Apache Spark and Python, a general-purpose, high-level programming language. \n",
        "\n",
        "**Tweepy:** Tweepy is a Python package that provides a convenient way to use the Twitter API. The Twitter API gives developers access to most of Twitter's functionality, such as reading and writing information related to tweets, users, and trends."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VABl_AhnJ5d6",
        "colab_type": "text"
      },
      "source": [
        "### 1.3. Sentiment Analysis Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYt-Mf4zJ5d7",
        "colab_type": "text"
      },
      "source": [
        "![sentiment_analysis_pipeline.png](attachment:sentiment_analysis_pipeline.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIJGVezJJ5d7",
        "colab_type": "text"
      },
      "source": [
        "# 2. Twitter Data Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGLle0EfJ5d7",
        "colab_type": "text"
      },
      "source": [
        "### 2.1. Streaming Twitter Data and Saving in .csv Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-M2_YgFJ5d8",
        "colab_type": "text"
      },
      "source": [
        "The following code was ran in a PythonAnywhere Cloud Server for hashtags, or keywords, associated with the NFL Draft. In order to access Twitter's API, I created an account on Algorithmia.com and then a Twitter developer account and went through the necessary steps to obtain access. I used multiple API calls in order to collect 50,000 total tweets for a span of five days, ranging from the first round of the draft (Day One) to two days following the third and final draft day. Due to the quantity of tweets utilizing the particular hashtags I used as keys, I had to scrape in separate .py files and then combine them before preprocessing here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRk1YAqLJ5d8",
        "colab_type": "raw"
      },
      "source": [
        "# Call the function scrapetweets\n",
        "scrapetweets(search_words, date_since, date_until, numTweets, numRuns) # Twitter credentials\n",
        "consumer_key = 'ZySgTb5HKxTZkqKzurkVW5ONG'\n",
        "consumer_secret = 'eMVDQwD25CM9EkE4ly9MkLjAvxUvKtk3zku4hZvuuYjp26XNwo'\n",
        "access_key = '1176869078738378753-VD2OWybAZHmXt0btIPHMn9ClB25ObD'\n",
        "access_secret = '1WxN4OoRvVloYnGGhZVpIzGvw9StYydLeBfliX6L9IXML'\n",
        "\n",
        "# Pass your twitter credentials to tweepy via its OAuthHandler\n",
        "auth = OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_key, access_secret)\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "# Calls API every 15 minutes to prevent overcalling\n",
        "\n",
        "def scrapetweets(search_words, date_since, date_until, numTweets, numRuns):\n",
        "\n",
        "    ## Arguments:\n",
        "    # search_words -> define a string of keywords for this function to extract\n",
        "    # date_since -> define a date from which to start extracting the tweets \n",
        "    # date_until -> define a date from which to end extracting the tweets \n",
        "    # numTweets -> number of tweets to extract per run\n",
        "    # numRun -> number of runs to perform in this program - API calls are limited to once every 15 mins, so each run will be 15 mins apart.\n",
        "    \n",
        "    # Define a pandas dataframe to store the date:\n",
        "    db_tweets = pd.DataFrame(columns = ['username', 'acctdesc', 'location', 'following',\n",
        "                                        'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts',\n",
        "                                        'retweetcount', 'text', 'hashtags']\n",
        "                                )\n",
        "    # Define a for-loop to generate tweets at regular intervals\n",
        "    for i in range(0, numRuns):\n",
        "        # We will time how long it takes to scrape tweets for each run:\n",
        "        start_run = time.time()\n",
        "        \n",
        "        # Collect tweets using the Cursor object\n",
        "        # .Cursor() returns an object that you can iterate or loop over to access the data collected.\n",
        "        # Each item in the iterator has various attributes that you can access to get information about each tweet\n",
        "        tweets = tweepy.Cursor(api.search, q=search_words, lang=\"en\", since=date_since, until=date_until, tweet_mode='extended').items(numTweets)\n",
        "\n",
        "        # Store these tweets into a python list\n",
        "        tweet_list = [tweet for tweet in tweets]\n",
        "\n",
        "        # Begin scraping the tweets individually:\n",
        "        noTweets = 0\n",
        "\n",
        "        for tweet in tweet_list:\n",
        "\n",
        "            # Pull the values\n",
        "            username = tweet.user.screen_name\n",
        "            acctdesc = tweet.user.description\n",
        "            location = tweet.user.location\n",
        "            following = tweet.user.friends_count\n",
        "            followers = tweet.user.followers_count\n",
        "            totaltweets = tweet.user.statuses_count\n",
        "            usercreatedts = tweet.user.created_at\n",
        "            tweetcreatedts = tweet.created_at\n",
        "            retweetcount = tweet.retweet_count\n",
        "            hashtags = tweet.entities['hashtags']\n",
        "\n",
        "            try:\n",
        "                text = tweet.retweeted_status.full_text\n",
        "            except AttributeError:  # Not a Retweet\n",
        "                text = tweet.full_text\n",
        "\n",
        "            # Add the 11 variables to the empty list - ith_tweet:\n",
        "            ith_tweet = [username, acctdesc, location, following, followers, totaltweets,\n",
        "                         usercreatedts, tweetcreatedts, retweetcount, text, hashtags]\n",
        "\n",
        "            # Append to dataframe - db_tweets\n",
        "            db_tweets.loc[len(db_tweets)] = ith_tweet\n",
        "\n",
        "            # increase counter - noTweets  \n",
        "            noTweets += 1\n",
        "        \n",
        "        # Run ended:\n",
        "        end_run = time.time()\n",
        "        duration_run = round(end_run-start_run, 2)\n",
        "        \n",
        "        print('no. of tweets scraped for run {} is {}'.format(i, noTweets))\n",
        "        print('time take for {} run to complete is {}'.format(i, duration_run))\n",
        "        \n",
        "        time.sleep(900) #15 minute sleep time\n",
        "\n",
        "        \n",
        "    # Once all runs have completed, save them to a single csv file:    \n",
        "    # Obtain timestamp in a readable format:\n",
        "    from datetime import datetime\n",
        "    to_csv_timestamp = datetime.today().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "    # Define working path and filename\n",
        "    path = os.getcwd()\n",
        "    filename = path + '/data/' + to_csv_timestamp + 'nfldraftsample.csv'\n",
        "\n",
        "    # Store dataframe in csv with creation date timestamp\n",
        "    db_tweets.to_csv(filename, index = False)\n",
        "    \n",
        "    print('Scraping has completed!')\n",
        "\n",
        "# Initialise these variables:\n",
        "search_words = \"#nfldraft OR #nfldraft2020 OR #nfldraftday OR #draft OR #draft2020\"\n",
        "date_since = \"2020-4-23\"\n",
        "date_until = \"2020-4-27\"\n",
        "numTweets = 2500\n",
        "numRuns = 20\n",
        "# Call the function scrapetweets\n",
        "scrapetweets(search_words, date_since, date_until, numTweets, numRuns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxOkNJZxJ5d9",
        "colab_type": "text"
      },
      "source": [
        "### 2.2. Key Attributes in Tweet .csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXSPvNydJ5d9",
        "colab_type": "text"
      },
      "source": [
        "**The following information was collected:**\n",
        "- user.screen_name - twitter handle\n",
        "- user.description - description of account\n",
        "- user.location - where is he tweeting from\n",
        "- user.friends_count - no. of other users that user is following (following)\n",
        "- user.followers_count - no. of other users who are following this user (followers)\n",
        "- user.statuses_count - total tweets by user\n",
        "- user.created_at - when the user account was created\n",
        "- created_at - when the tweet was created\n",
        "- retweet_count - no. of retweets\n",
        "- (deprecated) user.favourites_count - probably total no. of tweets that is favourited by user\n",
        "- retweeted_status.full_text - full text of the tweet\n",
        "- tweet.entities['hashtags'] - hashtags in the tweet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHUxiv13J5d9",
        "colab_type": "text"
      },
      "source": [
        "# 3. Loading Tweet Data Into PySpark DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXKFvInBJ5d-",
        "colab_type": "text"
      },
      "source": [
        "I have loaded the necessary Spark files into my computer and set paths in order to be able to spin up a Spark cluster on my local machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l27uipjdJ5d-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "11fa4713-672b-4568-a6f2-31baead85d89"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar -xvf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spark-2.4.5-bin-hadoop2.7/\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-jtransforms.html\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-zstd.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-zstd-jni.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-xmlenc.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-vis.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-spire.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-sorttable.js.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-slf4j.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-scopt.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-scala.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-sbt-launch-lib.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-respond.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-reflectasm.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-pyrolite.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-py4j.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-protobuf.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-pmml-model.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-paranamer.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-netlib.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-mustache.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-modernizr.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-minlog.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-matchMedia-polyfill.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-machinist.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-leveldbjni.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-kryo.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-json-formatter.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-jquery.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-join.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-jodd.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-jline.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-javolution.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-javassist.html\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-janino.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-heapq.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-graphlib-dot.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-f2j.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-datatables.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-dagre-d3.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-d3.min.js.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-cloudpickle.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-bootstrap.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-automaton.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-arpack.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-antlr.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-CC0.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-AnchorJS.txt\n",
            "spark-2.4.5-bin-hadoop2.7/LICENSE\n",
            "spark-2.4.5-bin-hadoop2.7/examples/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/jars/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/examples/jars/scopt_2.11-3.7.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRegressionMetricsExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLinearRegressionWithSGDExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPrefixSpanExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderEstimatorExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSizeHintExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaSummarizerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPowerIterationClusteringExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFeatureHasherExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaTC.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkRemoteFileTest.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RegressionMetricsExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LinearRegressionWithSGDExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LinearRegression.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PrefixSpanExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderEstimatorExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSizeHintExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/SummarizerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PowerIterationClusteringExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FeatureHasherExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/resources/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/resources/users.parquet\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/resources/users.orc\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/resources/users.avro\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/resources/user.avsc\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/resources/people.txt\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/resources/people.json\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/resources/people.csv\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/resources/kv1.txt\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/resources/full_user.avsc\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/resources/employees.json\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/RSparkSQLExample.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/streaming/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/streaming/structured_network_wordcount.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/svmLinear.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/survreg.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/randomForest.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/naiveBayes.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/mlp.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/ml.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/logit.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/lda.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/kstest.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/kmeans.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/isoreg.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/glm.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/gbt.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/gaussianMixture.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/fpm.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/decisionTree.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/bisectingKmeans.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/als.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/dataframe.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/data-manipulation.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/wordcount.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/transitive_closure.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/streaming/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/streaming/recoverable_network_wordcount.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/streaming/kafka_wordcount.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/streaming/flume_wordcount.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/streaming/direct_kafka_wordcount.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/streaming/stateful_network_wordcount.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/streaming/sql_network_wordcount.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/streaming/queue_stream.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/streaming/network_wordjoinsentiments.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/streaming/network_wordcount.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/streaming/hdfs_wordcount.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/status_api_demo.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/sql/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/sql/hive.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/sql/datasource.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/sql/arrow.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/sql/streaming/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/sql/basic.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/sort.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/pi.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/parquet_inputformat.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/pagerank.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/standard_scaler_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/ranking_metrics_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/multi_class_metrics_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/isotonic_regression_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/bisecting_k_means_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/word2vec_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/word2vec.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/tf_idf_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/svm_with_sgd_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/svd_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/summary_statistics_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/streaming_linear_regression_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/streaming_k_means_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/stratified_sampling_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/sampled_rdds.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/regression_metrics_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/recommendation_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/random_rdd_generation.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_regression_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_classification_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/power_iteration_clustering_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/pca_rowmatrix_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/normalizer_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/naive_bayes_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/multi_label_metrics_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/linear_regression_with_sgd_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/kmeans.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/kernel_density_estimation_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/k_means_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_regression_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_classification_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_model.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/fpgrowth_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/elementwise_product_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_regression_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_classification_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/correlations_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/correlations.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/binary_classification_metrics_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/onehot_encoder_estimator_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/bisecting_k_means_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/word2vec_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/vector_slicer_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/vector_size_hint_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/vector_indexer_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/vector_assembler_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/train_validation_split.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/tokenizer_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/tf_idf_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/summarizer_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/string_indexer_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/stopwords_remover_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/standard_scaler_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/sql_transformer.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/rformula_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/random_forest_regressor_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/random_forest_classifier_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/quantile_discretizer_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/prefixspan_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/polynomial_expansion_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/pipeline_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/pca_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/one_vs_rest_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/normalizer_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/naive_bayes_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/n_gram_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/multilayer_perceptron_classification.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/min_max_scaler_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/min_hash_lsh_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/max_abs_scaler_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_summary_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/linearsvc.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/linear_regression_with_elastic_net.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/lda_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/kmeans_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/isotonic_regression_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/index_to_string_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/imputer_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/generalized_linear_regression_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/gaussian_mixture_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/fpgrowth_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/feature_hasher_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/estimator_transformer_param_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/elementwise_product_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_regression_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_classification_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/dct_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/dataframe_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/cross_validator.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/count_vectorizer_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/correlation_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/chisq_selector_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/chi_square_test_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/bucketizer_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/binarizer_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/als_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/aft_survival_regression.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/logistic_regression.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/kmeans.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/avro_inputformat.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/als.py\n",
            "spark-2.4.5-bin-hadoop2.7/kubernetes/\n",
            "spark-2.4.5-bin-hadoop2.7/kubernetes/dockerfiles/\n",
            "spark-2.4.5-bin-hadoop2.7/kubernetes/dockerfiles/spark/\n",
            "spark-2.4.5-bin-hadoop2.7/kubernetes/dockerfiles/spark/entrypoint.sh\n",
            "spark-2.4.5-bin-hadoop2.7/kubernetes/dockerfiles/spark/Dockerfile\n",
            "spark-2.4.5-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/\n",
            "spark-2.4.5-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/python/\n",
            "spark-2.4.5-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/python/Dockerfile\n",
            "spark-2.4.5-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/R/\n",
            "spark-2.4.5-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/R/Dockerfile\n",
            "spark-2.4.5-bin-hadoop2.7/kubernetes/tests/\n",
            "spark-2.4.5-bin-hadoop2.7/kubernetes/tests/pyfiles.py\n",
            "spark-2.4.5-bin-hadoop2.7/kubernetes/tests/worker_memory_check.py\n",
            "spark-2.4.5-bin-hadoop2.7/kubernetes/tests/py_container_checks.py\n",
            "spark-2.4.5-bin-hadoop2.7/yarn/\n",
            "spark-2.4.5-bin-hadoop2.7/yarn/spark-2.4.5-yarn-shuffle.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/\n",
            "spark-2.4.5-bin-hadoop2.7/jars/scala-compiler-2.11.12.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/pyrolite-4.13.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/py4j-0.10.7.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/protobuf-java-2.5.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/parquet-jackson-1.10.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/parquet-hadoop-bundle-1.6.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/parquet-hadoop-1.10.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/parquet-format-2.4.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/parquet-encoding-1.10.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/parquet-common-1.10.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/parquet-column-1.10.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/paranamer-2.8.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/osgi-resource-locator-1.0.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/oro-2.0.8.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/orc-shims-1.5.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/orc-mapreduce-1.5.5-nohive.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/orc-core-1.5.5-nohive.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/opencsv-2.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/okio-1.15.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/okhttp-3.12.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/objenesis-2.5.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/netty-all-4.1.42.Final.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/netty-3.9.9.Final.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/minlog-1.3.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/metrics-jvm-3.1.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/metrics-json-3.1.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/metrics-graphite-3.1.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/metrics-core-3.1.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/mesos-1.4.0-shaded-protobuf.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/macro-compat_2.11-1.1.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/machinist_2.11-0.6.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/lz4-java-1.4.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/logging-interceptor-3.12.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/zstd-jni-1.3.2-2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/zookeeper-3.4.6.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/zjsonpatch-0.3.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/xz-1.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/xmlenc-0.52.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/xercesImpl-2.9.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/xbean-asm6-shaded-4.8.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/validation-api-1.1.0.Final.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/univocity-parsers-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/super-csv-2.2.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/stringtemplate-3.2.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/stream-2.7.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/stax-api-1.0.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/stax-api-1.0-2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spire_2.11-0.13.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spire-macros_2.11-0.13.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-yarn_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-unsafe_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-tags_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-tags_2.11-2.4.5-tests.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-streaming_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-sql_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-sketch_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-repl_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-network-shuffle_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-network-common_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-mllib_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-mllib-local_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-mesos_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-launcher_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-kvstore_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-kubernetes_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-hive_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-hive-thriftserver_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-graphx_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-core_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-catalyst_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/snappy-java-1.1.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/snappy-0.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/snakeyaml-1.15.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/slf4j-api-1.7.16.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/shims-0.7.45.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/shapeless_2.11-2.3.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/scala-xml_2.11-1.0.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/scala-reflect-2.11.12.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/scala-parser-combinators_2.11-1.1.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/scala-library-2.11.12.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/log4j-1.2.17.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/libthrift-0.9.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/libfb303-0.9.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/leveldbjni-all-1.8.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/kubernetes-model-common-4.6.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/kubernetes-model-4.6.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/kubernetes-client-4.6.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/kryo-shaded-4.0.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jul-to-slf4j-1.7.16.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jtransforms-2.4.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jta-1.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jsr305-1.3.9.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jsp-api-2.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/json4s-scalap_2.11-3.5.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/json4s-jackson_2.11-3.5.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/json4s-core_2.11-3.5.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/json4s-ast_2.11-3.5.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jpam-1.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jodd-core-3.5.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/joda-time-2.9.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jline-2.14.6.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jetty-util-6.1.26.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jetty-6.1.26.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jersey-server-2.22.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jersey-media-jaxb-2.22.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jersey-guava-2.22.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jersey-container-servlet-core-2.22.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jersey-container-servlet-2.22.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jersey-common-2.22.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jersey-client-2.22.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jdo-api-3.0.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jcl-over-slf4j-1.7.16.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jaxb-api-2.2.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/javolution-5.5.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/javax.ws.rs-api-2.0.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/javax.servlet-api-3.1.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/javax.inject-2.4.0-b34.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/javax.inject-1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/javax.annotation-api-1.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/javassist-3.18.1-GA.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/janino-3.0.9.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jackson-xc-1.9.13.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jackson-module-scala_2.11-2.6.7.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jackson-module-paranamer-2.7.9.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jackson-module-jaxb-annotations-2.6.7.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jackson-mapper-asl-1.9.13.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jackson-jaxrs-1.9.13.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jackson-dataformat-yaml-2.6.7.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jackson-databind-2.6.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jackson-core-asl-1.9.13.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jackson-core-2.6.7.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jackson-annotations-2.6.7.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/ivy-2.4.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/httpcore-4.4.10.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/httpclient-4.5.6.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/htrace-core-3.1.0-incubating.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hppc-0.7.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hk2-utils-2.4.0-b34.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hk2-locator-2.4.0-b34.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hk2-api-2.4.0-b34.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hive-metastore-1.2.1.spark2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hive-exec-1.2.1.spark2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hive-cli-1.2.1.spark2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hive-beeline-1.2.1.spark2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-yarn-server-web-proxy-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-yarn-server-common-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-yarn-common-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-yarn-client-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-yarn-api-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-mapreduce-client-shuffle-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-mapreduce-client-jobclient-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-mapreduce-client-core-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-mapreduce-client-common-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-mapreduce-client-app-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-hdfs-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-client-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-auth-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-annotations-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/guice-servlet-3.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/guice-3.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/guava-14.0.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/gson-2.2.4.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/generex-1.0.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/flatbuffers-1.2.0-3f79e055.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/eigenbase-properties-1.1.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/derby-10.12.1.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/datanucleus-rdbms-3.2.9.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/datanucleus-core-3.2.10.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/datanucleus-api-jdo-3.2.6.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/curator-recipes-2.7.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/curator-framework-2.7.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/curator-client-2.7.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/core-1.1.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/compress-lzf-1.0.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-pool-1.5.4.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-net-3.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-math3-3.4.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-logging-1.1.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-lang3-3.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-lang-2.6.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-io-2.4.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-httpclient-3.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-digester-1.8.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-dbcp-1.4.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-crypto-1.0.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-configuration-1.6.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-compress-1.8.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-compiler-3.0.9.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-collections-3.2.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-codec-1.10.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-cli-1.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-beanutils-1.9.4.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/chill_2.11-0.9.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/chill-java-0.9.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/calcite-linq4j-1.2.0-incubating.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/calcite-core-1.2.0-incubating.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/calcite-avatica-1.2.0-incubating.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/breeze_2.11-0.13.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/breeze-macros_2.11-0.13.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/bonecp-0.8.0.RELEASE.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/avro-mapred-1.8.2-hadoop2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/avro-ipc-1.8.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/avro-1.8.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/automaton-1.11-8.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/arrow-vector-0.10.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/arrow-memory-0.10.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/arrow-format-0.10.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/arpack_combined_all-0.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/api-util-1.0.0-M20.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/api-asn1-api-1.0.0-M20.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/apacheds-kerberos-codec-2.0.0-M15.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/apacheds-i18n-2.0.0-M15.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/apache-log4j-extras-1.2.17.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/aopalliance-repackaged-2.4.0-b34.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/aopalliance-1.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/antlr4-runtime-4.7.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/antlr-runtime-3.4.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/antlr-2.7.7.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/aircompressor-0.10.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/activation-1.1.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/ST4-4.0.4.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/RoaringBitmap-0.7.45.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/JavaEWAH-0.3.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/RELEASE\n",
            "spark-2.4.5-bin-hadoop2.7/R/\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/sparkr.zip\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/INDEX\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/html/\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/html/R.css\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/html/00Index.html\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/help/\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/help/aliases.rds\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/help/AnIndex\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdx\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdb\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/help/paths.rds\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/worker/\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/worker/worker.R\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/worker/daemon.R\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/tests/\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/tests/testthat/\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/tests/testthat/test_basic.R\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/profile/\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/profile/shell.R\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/profile/general.R\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/R/\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdx\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdb\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/R/SparkR\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/Meta/\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/Meta/nsInfo.rds\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/Meta/links.rds\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/Meta/hsearch.rds\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/Meta/Rd.rds\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/Meta/features.rds\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/Meta/package.rds\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/DESCRIPTION\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/NAMESPACE\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/stop-shuffle-service.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/start-thriftserver.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/start-slave.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/start-shuffle-service.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/start-mesos-shuffle-service.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/start-master.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/start-history-server.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/spark-config.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/stop-thriftserver.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/stop-slaves.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/stop-slave.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/stop-mesos-shuffle-service.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/stop-mesos-dispatcher.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/stop-master.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/stop-history-server.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/stop-all.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/start-slaves.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/start-mesos-dispatcher.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/start-all.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/spark-daemons.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/spark-daemon.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/slaves.sh\n",
            "spark-2.4.5-bin-hadoop2.7/python/\n",
            "spark-2.4.5-bin-hadoop2.7/python/dist/\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark.egg-info/\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark.egg-info/SOURCES.txt\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark.egg-info/dependency_links.txt\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark.egg-info/top_level.txt\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark.egg-info/PKG-INFO\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark.egg-info/requires.txt\n",
            "spark-2.4.5-bin-hadoop2.7/python/setup.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/run-tests.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/run-tests-with-coverage\n",
            "spark-2.4.5-bin-hadoop2.7/python/run-tests\n",
            "spark-2.4.5-bin-hadoop2.7/python/README.md\n",
            "spark-2.4.5-bin-hadoop2.7/python/MANIFEST.in\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/userlibrary.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/userlib-0.1.zip\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/text-test.txt\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/streaming/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/streaming/text-test.txt\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/people_array_utf16le.json\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/people_array.json\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/people1.json\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/people.json\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/part-r-00007.gz.parquet\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/.part-r-00007.gz.parquet.crc\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/part-r-00005.gz.parquet\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/.part-r-00005.gz.parquet.crc\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00004.gz.parquet\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00002.gz.parquet\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00004.gz.parquet.crc\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00002.gz.parquet.crc\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/part-r-00008.gz.parquet\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/.part-r-00008.gz.parquet.crc\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_metadata\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_common_metadata\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_SUCCESS\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/orc_partitioned/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/orc_partitioned/_SUCCESS\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/ages_newlines.csv\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/ages.csv\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/hello/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/hello/sub_hello/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/hello/sub_hello/sub_hello.txt\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/hello/hello.txt\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/SimpleHTTPServer.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_coverage/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_coverage/sitecustomize.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_coverage/coverage_daemon.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_coverage/conf/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_coverage/conf/spark-defaults.conf\n",
            "spark-2.4.5-bin-hadoop2.7/python/setup.cfg\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/python/\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/python/pyspark/\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/python/pyspark/shell.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/worker.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/version.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/util.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/tests.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/test_serializers.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/test_broadcast.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/taskcontext.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/storagelevel.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/shuffle.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/serializers.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/resultiterable.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/rdd.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/profiler.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/java_gateway.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/files.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/daemon.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/context.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/conf.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/cloudpickle.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/broadcast.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/accumulators.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/__init__.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/traceback_utils.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/streaming/\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/streaming/tests.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/streaming/kinesis.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/streaming/kafka.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/streaming/flume.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/streaming/dstream.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/streaming/context.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/streaming/util.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/streaming/listener.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/streaming/__init__.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/status.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/statcounter.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/window.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/utils.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/udf.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/types.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/tests.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/streaming.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/session.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/readwriter.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/group.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/functions.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/dataframe.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/context.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/column.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/catalog.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/__init__.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/conf.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/shell.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/rddsampler.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/util.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/tree.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/tests.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/regression.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/recommendation.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/random.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/fpm.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/feature.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/evaluation.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/clustering.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/classification.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/stat/\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/stat/_statistics.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/stat/test.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/stat/distribution.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/stat/__init__.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/stat/KernelDensity.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/linalg/\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/linalg/distributed.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/linalg/__init__.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/common.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/__init__.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/wrapper.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/util.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/tuning.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/tests.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/stat.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/regression.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/recommendation.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/image.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/fpm.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/feature.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/evaluation.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/clustering.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/classification.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/base.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/pipeline.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/param/\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/param/shared.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/param/_shared_params_code_gen.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/param/__init__.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/linalg/\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/linalg/__init__.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/common.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/__init__.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/join.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/heapq3.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/find_spark_home.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/_globals.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pylintrc\n",
            "spark-2.4.5-bin-hadoop2.7/python/lib/\n",
            "spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip\n",
            "spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip\n",
            "spark-2.4.5-bin-hadoop2.7/python/lib/PY4J_LICENSE.txt\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/pyspark.streaming.rst\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/pyspark.sql.rst\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/pyspark.ml.rst\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/epytext.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/conf.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/Makefile\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/pyspark.rst\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/pyspark.mllib.rst\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/make2.bat\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/make.bat\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/index.rst\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/_templates/\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/_templates/layout.html\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/_static/\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/_static/pyspark.js\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/_static/pyspark.css\n",
            "spark-2.4.5-bin-hadoop2.7/python/.gitignore\n",
            "spark-2.4.5-bin-hadoop2.7/python/.coveragerc\n",
            "spark-2.4.5-bin-hadoop2.7/bin/\n",
            "spark-2.4.5-bin-hadoop2.7/bin/spark-class\n",
            "spark-2.4.5-bin-hadoop2.7/bin/pyspark2.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/pyspark\n",
            "spark-2.4.5-bin-hadoop2.7/bin/load-spark-env.sh\n",
            "spark-2.4.5-bin-hadoop2.7/bin/load-spark-env.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/docker-image-tool.sh\n",
            "spark-2.4.5-bin-hadoop2.7/bin/sparkR2.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/sparkR.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/sparkR\n",
            "spark-2.4.5-bin-hadoop2.7/bin/spark-submit2.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/spark-submit.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/spark-submit\n",
            "spark-2.4.5-bin-hadoop2.7/bin/spark-sql2.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/spark-sql.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/spark-sql\n",
            "spark-2.4.5-bin-hadoop2.7/bin/spark-shell2.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/spark-shell.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/spark-shell\n",
            "spark-2.4.5-bin-hadoop2.7/bin/spark-class2.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/spark-class.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/run-example.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/run-example\n",
            "spark-2.4.5-bin-hadoop2.7/bin/pyspark.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/find-spark-home.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/find-spark-home\n",
            "spark-2.4.5-bin-hadoop2.7/bin/beeline.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/beeline\n",
            "spark-2.4.5-bin-hadoop2.7/README.md\n",
            "spark-2.4.5-bin-hadoop2.7/conf/\n",
            "spark-2.4.5-bin-hadoop2.7/conf/spark-env.sh.template\n",
            "spark-2.4.5-bin-hadoop2.7/conf/spark-defaults.conf.template\n",
            "spark-2.4.5-bin-hadoop2.7/conf/slaves.template\n",
            "spark-2.4.5-bin-hadoop2.7/conf/metrics.properties.template\n",
            "spark-2.4.5-bin-hadoop2.7/conf/log4j.properties.template\n",
            "spark-2.4.5-bin-hadoop2.7/conf/fairscheduler.xml.template\n",
            "spark-2.4.5-bin-hadoop2.7/conf/docker.properties.template\n",
            "spark-2.4.5-bin-hadoop2.7/data/\n",
            "spark-2.4.5-bin-hadoop2.7/data/streaming/\n",
            "spark-2.4.5-bin-hadoop2.7/data/streaming/AFINN-111.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/streaming_kmeans_data_test.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/sample_svm_data.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/sample_multiclass_classification_data.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/sample_movielens_data.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/sample_linear_regression_data.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/sample_lda_libsvm_data.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/sample_lda_data.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/sample_kmeans_data.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/sample_isotonic_regression_libsvm_data.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/sample_fpgrowth.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/sample_binary_classification_data.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/ridge-data/\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/ridge-data/lpsa.data\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/pic_data.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/pagerank_data.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/kmeans_data.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/iris_libsvm.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/grayscale.jpg\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/chr30.4.184.jpg\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA_alpha_60.png\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA.png\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP802813.jpg\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP153539.jpg\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/54893.jpg\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/not-image.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/29.5.a_b_EGDP022204.jpg\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/origin/\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/origin/multi-channel/\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/origin/multi-channel/grayscale.jpg\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/origin/multi-channel/chr30.4.184.jpg\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/origin/multi-channel/BGRA_alpha_60.png\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/origin/multi-channel/BGRA.png\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/origin/license.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/origin/kittens/\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/origin/kittens/not-image.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/origin/kittens/DP802813.jpg\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/origin/kittens/DP153539.jpg\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/origin/kittens/54893.jpg\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/origin/kittens/29.5.a_b_EGDP022204.jpg\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/license.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/gmm_data.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/als/\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/als/test.data\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/als/sample_movielens_ratings.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/graphx/\n",
            "spark-2.4.5-bin-hadoop2.7/data/graphx/users.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/graphx/followers.txt\n",
            "spark-2.4.5-bin-hadoop2.7/NOTICE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K27jLmbFKCMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "\n",
        "def create_spark_context():\n",
        "  return SparkContext.getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67cFLfbuKFfA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext, SparkSession\n",
        "\n",
        "# Setting spark.driver.memory due to issues faced with pyspark.ml processing\n",
        "SparkContext.setSystemProperty('spark.driver.memory', '16g')\n",
        "\n",
        "# Create Spark Contexts\n",
        "sc = pyspark.SparkContext()\n",
        "sc.getConf()\n",
        "conf = pyspark.SparkConf().setAll([('spark.executor.memory', '16g'), ('spark.executor.cores', '3'), ('spark.cores.max', '3'), ('spark.driver.memory','8g'), (\"spark.memory.offHeap.size\",\"16g\"),('spark.memory.offHeap.enabled','true')])\n",
        "sc.stop()\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "sqlContext = SQLContext(sc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bgw-QndNJ5eB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "b07f0a41-caa6-4d2f-bb32-175bbd13c453"
      },
      "source": [
        "sc._conf.getAll()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spark.app.id', 'local-1589576794460'),\n",
              " ('spark.executor.id', 'driver'),\n",
              " ('spark.driver.port', '44029'),\n",
              " ('spark.memory.offHeap.enabled', 'true'),\n",
              " ('spark.app.name', 'pyspark-shell'),\n",
              " ('spark.cores.max', '3'),\n",
              " ('spark.executor.memory', '16g'),\n",
              " ('spark.rdd.compress', 'True'),\n",
              " ('spark.driver.memory', '8g'),\n",
              " ('spark.serializer.objectStreamReset', '100'),\n",
              " ('spark.master', 'local[*]'),\n",
              " ('spark.executor.cores', '3'),\n",
              " ('spark.submit.deployMode', 'client'),\n",
              " ('spark.ui.showConsoleProgress', 'true'),\n",
              " ('spark.memory.offHeap.size', '16g'),\n",
              " ('spark.driver.host', 'b16b9fa719b0')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dM_8DYBOKPPq",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "a81fbcf5-fb90-47f5-a697-dabe14b40c1f"
      },
      "source": [
        "# Import NFL Draft Tweets data\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-dd318f2b-9979-4609-986b-56a3cc1a7ad9\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-dd318f2b-9979-4609-986b-56a3cc1a7ad9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving NFLDraftTweets.csv to NFLDraftTweets.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k04D_C5sJ5eD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweetdf = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('NFLDraftTweets.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxaHjn7sSCyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head -1000 NFLDraftTweets.csv >> NFLDraftTweets_small.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbOGCgOCJ5eF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "420a3c04-8a0a-49ed-abf1-cf91e3cca407"
      },
      "source": [
        "type(tweetdf)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgtrxcXlJ5eH",
        "colab_type": "text"
      },
      "source": [
        "Above, we can see that this is in a PySpark DataFrame data structure. DataFrames in Spark are immutable, distributed, and designed to process structured data. To help Apache Spark understand the schema, the data is organized under named columns. This helps Spark optimize execution plans and handle pedabytes of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cWKJgLeJ5eI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5d76286c-36ab-45ef-dcf6-ed5ab89e7187"
      },
      "source": [
        "#Display first five rows of data\n",
        "tweetdf.show(5)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+---------------+---------+---------+---------------+----------------+---------------+--------------------+--------------------+--------------------+\n",
            "|            username|            acctdesc|       location|following|followers|    totaltweets|   usercreatedts| tweetcreatedts|        retweetcount|                text|            hashtags|\n",
            "+--------------------+--------------------+---------------+---------+---------+---------------+----------------+---------------+--------------------+--------------------+--------------------+\n",
            "|     Spectpooheagles|Optimuspooh, Mixe...|Philadelphia,pa|     1948|     1337|         206944|  4/26/2011 2:05|4/27/2020 23:59|                  52|#NFL #NFLDraft Te...|[{'text': 'NFL', ...|\n",
            "|         sammydabber|st rose class of ...|           null|     null|     null|           null|            null|           null|                null|                null|                null|\n",
            "|I like carmella m...|       United States|            470|       60|    25261|3/14/2019 20:12| 4/27/2020 23:59|            112|Moments after bei...|                null|                null|\n",
            "|#GoBills | #NFLDr...|[{'text': 'GoBill...|           null|     null|     null|           null|            null|           null|                null|                null|                null|\n",
            "|             LDW1884|                null|           null|     4394|     1007|          56953|11/29/2014 18:14|4/27/2020 23:59|                 642|The first wide re...|                null|\n",
            "+--------------------+--------------------+---------------+---------+---------+---------------+----------------+---------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COmdhICpJ5eK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "2386c2eb-f8ce-41e0-e0c6-e5bba37ff617"
      },
      "source": [
        "tweetdf.printSchema()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- username: string (nullable = true)\n",
            " |-- acctdesc: string (nullable = true)\n",
            " |-- location: string (nullable = true)\n",
            " |-- following: string (nullable = true)\n",
            " |-- followers: string (nullable = true)\n",
            " |-- totaltweets: string (nullable = true)\n",
            " |-- usercreatedts: string (nullable = true)\n",
            " |-- tweetcreatedts: string (nullable = true)\n",
            " |-- retweetcount: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- hashtags: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "030MutfuJ5eM",
        "colab_type": "text"
      },
      "source": [
        "# 4. Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVgpeKVDJ5eM",
        "colab_type": "text"
      },
      "source": [
        "### 4.1. Creating New DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm-hpkZyJ5eM",
        "colab_type": "text"
      },
      "source": [
        "Before applying performing sentiment analysis or applying a classification algorithm, we need to preprocess the data. First, I will create a new dataframe containing only the important columns we will be looking at in this project (text and hashtags). Then, I will perform custom transformations by creating Spark's User-Defined Functions (UDFs) for different common preprocessing techniques. I will be using the Natural Language Toolkit (NLTK) Python Package to do some of the preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "eptKSRI5J5eN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6674a129-b3c0-4405-d48a-852eafc19e8d"
      },
      "source": [
        "#Create newtweetdf with text and hashtag columns\n",
        "newtweetdf = tweetdf.drop('username', 'acctdesc', 'location', 'following', 'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts', 'retweetcount')\n",
        "\n",
        "#Drop duplicate rows\n",
        "newtweetdf = newtweetdf.dropDuplicates()\n",
        "\n",
        "#Drop rows with only null values\n",
        "newtweetdf = newtweetdf.dropna(how='all')\n",
        "newtweetdf.show(5)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+\n",
            "|                text|hashtags|\n",
            "+--------------------+--------+\n",
            "|How did you @Bill...|      []|\n",
            "|RD 2 | Pick 41 - ...|    null|\n",
            "|Nice local haul i...|    null|\n",
            "|      Dear @Raiders,|    null|\n",
            "|Garrett Taylor (@...|    null|\n",
            "+--------------------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep7ESMekJ5eP",
        "colab_type": "text"
      },
      "source": [
        "### 4.2. Remove Non-ASCII Characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAjZFS_4J5eP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "b9128fda-57cb-4948-f44b-9829f98d3232"
      },
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "set(stopwords.words(\"english\"))\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk import pos_tag\n",
        "import string\n",
        "import re"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKfVBIghJ5eR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def strip_non_ascii(data_str):\n",
        "    ''' Returns the string without non ASCII characters'''\n",
        "    stripped = (c for c in data_str if 0 < ord(c) < 127)\n",
        "    return ''.join(stripped)\n",
        "\n",
        "# Setup PySpark UDF Function\n",
        "strip_non_ascii_udf = udf(strip_non_ascii, StringType())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjxONxRSJ5eS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "aa12ca9b-83a2-4b77-d05e-2f80f113c45b"
      },
      "source": [
        "nonascii_df = newtweetdf.withColumn('text_non_asci',strip_non_ascii_udf(newtweetdf['text']))\n",
        "nonascii_df.show(5)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+--------------------+\n",
            "|                text|hashtags|       text_non_asci|\n",
            "+--------------------+--------+--------------------+\n",
            "|How did you @Bill...|      []|How did you @Bill...|\n",
            "|RD 2 | Pick 41 - ...|    null|RD 2 | Pick 41 - ...|\n",
            "|Nice local haul i...|    null|Nice local haul i...|\n",
            "|      Dear @Raiders,|    null|      Dear @Raiders,|\n",
            "|Garrett Taylor (@...|    null|Garrett Taylor (@...|\n",
            "+--------------------+--------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeK2Sk-SJ5eV",
        "colab_type": "text"
      },
      "source": [
        "### 4.3. Fix Abbreviations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6qfH0IZJ5eV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fix_abbreviation(data_str):\n",
        "    data_str = data_str.lower()\n",
        "    data_str = re.sub(r'\\bthats\\b', 'that is', data_str)\n",
        "    data_str = re.sub(r'\\bive\\b', 'i have', data_str)\n",
        "    data_str = re.sub(r'\\bim\\b', 'i am', data_str)\n",
        "    data_str = re.sub(r'\\bya\\b', 'yeah', data_str)\n",
        "    data_str = re.sub(r'\\bcant\\b', 'can not', data_str)\n",
        "    data_str = re.sub(r'\\bdont\\b', 'do not', data_str)\n",
        "    data_str = re.sub(r'\\bwont\\b', 'will not', data_str)\n",
        "    data_str = re.sub(r'\\bid\\b', 'i would', data_str)\n",
        "    data_str = re.sub(r'\\bwth\\b', 'what the hell', data_str)\n",
        "    data_str = re.sub(r'\\br\\b', 'are', data_str)\n",
        "    data_str = re.sub(r'\\bu\\b', 'you', data_str)\n",
        "    data_str = re.sub(r'\\bk\\b', 'OK', data_str)\n",
        "    data_str = re.sub(r'\\bsux\\b', 'sucks', data_str)\n",
        "    data_str = re.sub(r'\\bno+\\b', 'no', data_str)\n",
        "    data_str = re.sub(r'\\bcoo+\\b', 'cool', data_str)\n",
        "    data_str = re.sub(r'rt\\b', '', data_str)\n",
        "    data_str = data_str.strip()\n",
        "    return data_str\n",
        "\n",
        "# Setup PySpark UDF Function\n",
        "fix_abbreviation_udf = udf(fix_abbreviation, StringType())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjVlfCPPJ5eX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "196b8d19-d8cb-463c-98ac-371028691666"
      },
      "source": [
        "fixabb_df = nonascii_df.withColumn('fixed_abbrev',fix_abbreviation_udf(nonascii_df['text_non_asci']))\n",
        "fixabb_df.show(5)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+--------------------+--------------------+\n",
            "|                text|hashtags|       text_non_asci|        fixed_abbrev|\n",
            "+--------------------+--------+--------------------+--------------------+\n",
            "|How did you @Bill...|      []|How did you @Bill...|how did you @bill...|\n",
            "|RD 2 | Pick 41 - ...|    null|RD 2 | Pick 41 - ...|rd 2 | pick 41 - ...|\n",
            "|Nice local haul i...|    null|Nice local haul i...|nice local haul i...|\n",
            "|      Dear @Raiders,|    null|      Dear @Raiders,|      dear @raiders,|\n",
            "|Garrett Taylor (@...|    null|Garrett Taylor (@...|garrett taylor (@...|\n",
            "+--------------------+--------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "686DVARUJ5eZ",
        "colab_type": "text"
      },
      "source": [
        "### 4.4. Remove Stop Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSA435kIJ5eZ",
        "colab_type": "text"
      },
      "source": [
        "Stop words are some of the most common words in English like \"a\", \"the\", \"is\", etc. They are generally removed from text because they do not carry any sentiment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxX1N3JOJ5ea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_stops(data_str):\n",
        "    # expects a string\n",
        "    stops = stopwords.words()\n",
        "    list_pos = 0\n",
        "    cleaned_str = ''\n",
        "    text = data_str.split()\n",
        "    for word in text:\n",
        "        if word not in stops:\n",
        "            # rebuild cleaned_str\n",
        "            if list_pos == 0:\n",
        "                cleaned_str = word\n",
        "            else:\n",
        "                cleaned_str = cleaned_str + ' ' + word\n",
        "            list_pos += 1\n",
        "    return cleaned_str\n",
        "\n",
        "# Setup PySpark UDF function\n",
        "remove_stops_udf = udf(remove_stops, StringType())\n",
        "\n",
        "#Get raw columns\n",
        "raw_cols = fixabb_df.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03bBZCc6J5eb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c4d50cd5-e904-4a3b-9a54-df941770047f"
      },
      "source": [
        "stopwordsdf = fixabb_df.select(raw_cols).withColumn(\"stopword_text\", remove_stops_udf(fixabb_df[\"fixed_abbrev\"]))\n",
        "stopwordsdf.show(5)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+--------------------+--------------------+--------------------+\n",
            "|                text|hashtags|       text_non_asci|        fixed_abbrev|       stopword_text|\n",
            "+--------------------+--------+--------------------+--------------------+--------------------+\n",
            "|How did you @Bill...|      []|How did you @Bill...|how did you @bill...|@billgates go dev...|\n",
            "|RD 2 | Pick 41 - ...|    null|RD 2 | Pick 41 - ...|rd 2 | pick 41 - ...|rd 2 | pick 41 - ...|\n",
            "|Nice local haul i...|    null|Nice local haul i...|nice local haul i...|nice local haul #...|\n",
            "|      Dear @Raiders,|    null|      Dear @Raiders,|      dear @raiders,|      dear @raiders,|\n",
            "|Garrett Taylor (@...|    null|Garrett Taylor (@...|garrett taylor (@...|garrett taylor (@...|\n",
            "+--------------------+--------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4XpQ5SdJ5ed",
        "colab_type": "text"
      },
      "source": [
        "### 4.3. Remove Irrelevant Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMtnw_12J5ee",
        "colab_type": "text"
      },
      "source": [
        "Here, I'll remove various features from the such such as hyperlinks, mentions, short words, and punctuation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQuz8PUGJ5ee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_features(data_str):\n",
        "    # Compile regex\n",
        "    url_re = re.compile('https?://(www.)?\\w+\\.\\w+(/\\w+)*/?')\n",
        "    punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    num_re = re.compile('(\\\\d+)')\n",
        "    mention_re = re.compile('@(\\w+)')\n",
        "    alpha_num_re = re.compile(\"^[a-z0-9_.]+$\")\n",
        "    # Remove hyperlinks\n",
        "    data_str = url_re.sub(' ', data_str)\n",
        "    # Remove @mentions\n",
        "    data_str = mention_re.sub(' ', data_str)\n",
        "    # Remove puncuation\n",
        "    data_str = punc_re.sub(' ', data_str)\n",
        "    # Remove numeric 'words'\n",
        "    data_str = num_re.sub(' ', data_str)\n",
        "    # Remove non a-z 0-9 characters and words shorter than 3 characters\n",
        "    list_pos = 0\n",
        "    cleaned_str = ''\n",
        "    for word in data_str.split():\n",
        "        if list_pos == 0:\n",
        "            if alpha_num_re.match(word) and len(word) > 2:\n",
        "                cleaned_str = word\n",
        "            else:\n",
        "                cleaned_str = ' '\n",
        "        else:\n",
        "            if alpha_num_re.match(word) and len(word) > 2:\n",
        "                cleaned_str = cleaned_str + ' ' + word\n",
        "            else:\n",
        "                cleaned_str += ' '\n",
        "        list_pos += 1\n",
        "    return cleaned_str\n",
        "\n",
        "#Setup PySpark UDF Function\n",
        "remove_features_udf = udf(remove_features, StringType())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STWCmV2CJ5ef",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "74c8468d-a90e-4afc-f6d6-e8d2b5471d5a"
      },
      "source": [
        "rem_features_df = stopwordsdf.select(raw_cols+[\"stopword_text\"]).withColumn(\"rem_feat_text\", remove_features_udf(stopwordsdf[\"stopword_text\"]))\n",
        "rem_features_df.show(5)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|hashtags|       text_non_asci|        fixed_abbrev|       stopword_text|       rem_feat_text|\n",
            "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+\n",
            "|How did you @Bill...|      []|How did you @Bill...|how did you @bill...|@billgates go dev...|  developing code...|\n",
            "|RD 2 | Pick 41 - ...|    null|RD 2 | Pick 41 - ...|rd 2 | pick 41 - ...|rd 2 | pick 41 - ...|  pick jonathan t...|\n",
            "|Nice local haul i...|    null|Nice local haul i...|nice local haul i...|nice local haul #...|nice local haul n...|\n",
            "|      Dear @Raiders,|    null|      Dear @Raiders,|      dear @raiders,|      dear @raiders,|                dear|\n",
            "|Garrett Taylor (@...|    null|Garrett Taylor (@...|garrett taylor (@...|garrett taylor (@...|garrett taylor he...|\n",
            "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo840s3HJ5eh",
        "colab_type": "text"
      },
      "source": [
        "### 4.4. Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSlcS3-LJ5eh",
        "colab_type": "text"
      },
      "source": [
        "The goal of lemmmatization is to remove inflections and map a word to it's root form. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiEqa2Z7J5ei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lemmatize(data_str):\n",
        "    # expects a string\n",
        "    list_pos = 0\n",
        "    cleaned_str = ''\n",
        "    lmtzr = WordNetLemmatizer()\n",
        "    text = data_str.split()\n",
        "    tagged_words = pos_tag(text)\n",
        "    for word in tagged_words:\n",
        "        if 'v' in word[1].lower():\n",
        "            lemma = lmtzr.lemmatize(word[0], pos='v')\n",
        "        else:\n",
        "            lemma = lmtzr.lemmatize(word[0], pos='n')\n",
        "        if list_pos == 0:\n",
        "            cleaned_str = lemma\n",
        "        else:\n",
        "            cleaned_str = cleaned_str + ' ' + lemma\n",
        "        list_pos += 1\n",
        "    return cleaned_str\n",
        "\n",
        "# Setup PySpark UDF Function\n",
        "lemmatize_udf = udf(lemmatize, StringType())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0eWlwkmJ5ej",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "3f1d511d-123b-4051-d251-977c9ada86e4"
      },
      "source": [
        "lemm_df = rem_features_df.select(raw_cols+[\"rem_feat_text\"]).withColumn(\"cleaned_text\", lemmatize_udf(rem_features_df[\"rem_feat_text\"]))\n",
        "lemm_df.show(5)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|hashtags|       text_non_asci|        fixed_abbrev|       rem_feat_text|        cleaned_text|\n",
            "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+\n",
            "|How did you @Bill...|      []|How did you @Bill...|how did you @bill...|  developing code...|develop code comp...|\n",
            "|RD 2 | Pick 41 - ...|    null|RD 2 | Pick 41 - ...|rd 2 | pick 41 - ...|  pick jonathan t...|pick jonathan tay...|\n",
            "|Nice local haul i...|    null|Nice local haul i...|nice local haul i...|nice local haul n...|nice local haul n...|\n",
            "|      Dear @Raiders,|    null|      Dear @Raiders,|      dear @raiders,|                dear|                dear|\n",
            "|Garrett Taylor (@...|    null|Garrett Taylor (@...|garrett taylor (@...|garrett taylor he...|garrett taylor he...|\n",
            "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdaj927JJ5el",
        "colab_type": "text"
      },
      "source": [
        "### 4.5. Create Cleaned DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7z8UifZJ5el",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f48addfa-3843-4154-cb20-694e3f14bc7d"
      },
      "source": [
        "data = lemm_df.select('cleaned_text','hashtags')\n",
        "data.show(5)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+\n",
            "|        cleaned_text|hashtags|\n",
            "+--------------------+--------+\n",
            "|develop code comp...|      []|\n",
            "|pick jonathan tay...|    null|\n",
            "|nice local haul n...|    null|\n",
            "|                dear|    null|\n",
            "|garrett taylor he...|    null|\n",
            "+--------------------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f185n68NJ5eo",
        "colab_type": "text"
      },
      "source": [
        "# 5. Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3X0GdCDJ5eo",
        "colab_type": "text"
      },
      "source": [
        "Sentiment Analysis is the process of 'computationally' determining whether a piece of writing is positive, negative, or neutral in order to try and determine the attitude of the writer. It's widely used by businesses for analyzing customer reviews, social media, and survey responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tIQinbWJ5eo",
        "colab_type": "text"
      },
      "source": [
        "### 5.1. Creating the Sentiment Analysis Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWtmYYtGJ5ep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.types import FloatType\n",
        "from textblob import TextBlob\n",
        "\n",
        "def sentiment_analysis(text):\n",
        "    return TextBlob(text).sentiment.polarity\n",
        "\n",
        "# Setup PySpark UDF Function\n",
        "sentiment_analysis_udf = udf(sentiment_analysis , FloatType())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShA8c8VyJ5eq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "7c2a00e9-c765-4a5e-cb12-aa65b50bdefd"
      },
      "source": [
        "data = data.withColumn(\"sentiment_score\", sentiment_analysis_udf(data['cleaned_text']))\n",
        "data.show(5)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+---------------+\n",
            "|        cleaned_text|hashtags|sentiment_score|\n",
            "+--------------------+--------+---------------+\n",
            "|develop code comp...|      []|            0.0|\n",
            "|pick jonathan tay...|    null|            0.0|\n",
            "|nice local haul n...|    null|            0.3|\n",
            "|                dear|    null|            0.0|\n",
            "|garrett taylor he...|    null|            0.0|\n",
            "+--------------------+--------+---------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Xpav8p9J5es",
        "colab_type": "text"
      },
      "source": [
        "### 5.2. Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYVQOcLgJ5es",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def condition(r):\n",
        "    if (r >=0.1):\n",
        "        label = \"positive\"\n",
        "    elif(r <= -0.1):\n",
        "        label = \"negative\"\n",
        "    else:\n",
        "        label = \"neutral\"\n",
        "    return label\n",
        "\n",
        "# Setup PySpark UDF Function\n",
        "sentiment_udf = udf(lambda x: condition(x), StringType())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RKx7BrSJ5et",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6ba3dfb4-467d-492c-a695-d05c9c3c0f3c"
      },
      "source": [
        "data = data.withColumn(\"sentiment\", sentiment_udf(data['sentiment_score']))\n",
        "data.show(5)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+---------------+---------+\n",
            "|        cleaned_text|hashtags|sentiment_score|sentiment|\n",
            "+--------------------+--------+---------------+---------+\n",
            "|develop code comp...|      []|            0.0|  neutral|\n",
            "|pick jonathan tay...|    null|            0.0|  neutral|\n",
            "|nice local haul n...|    null|            0.3| positive|\n",
            "|                dear|    null|            0.0|  neutral|\n",
            "|garrett taylor he...|    null|            0.0|  neutral|\n",
            "+--------------------+--------+---------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si2aBDLhJ5ev",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "06b8f9e1-b1c6-4867-af50-eb5f758df0eb"
      },
      "source": [
        "data.groupBy(\"sentiment\").count().orderBy(\"sentiment\").show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+-----+\n",
            "|sentiment|count|\n",
            "+---------+-----+\n",
            "| negative|  171|\n",
            "|  neutral| 1238|\n",
            "| positive|  748|\n",
            "+---------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5SidCdmJ5ex",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a015118d-07ef-494d-c771-34e2fd884001"
      },
      "source": [
        "# Total cleaned dataset length\n",
        "print(data.count())\n",
        "\n",
        "# Percentages\n",
        "print(171/2157)\n",
        "print(1238/2157)\n",
        "print(748/2157)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2157\n",
            "0.07927677329624479\n",
            "0.573945294390357\n",
            "0.34677793231339826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Km78UdOQkwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.write.csv('CleanedTweets.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mW35qkVRhxW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e3b42cac-1db6-41b1-8f5b-5fd87ccabb07"
      },
      "source": [
        "!head -1000 data >> CleanedTweets"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "head: cannot open 'data' for reading: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2btwUnQ9J5e1",
        "colab_type": "text"
      },
      "source": [
        "**Results:** The first impression of these results is that the data cleaning process negated a ton of our initial scraped dataset. This could be due to there being a lot of null values, duplicates, or incorrectly formatted tweet data during the scraping process. The percentage breakdown for each of the sentiment categories within the dataset are as follows:\n",
        "\n",
        "- **Negative: 7.93%**\n",
        "- **Neutral: 57.39%**\n",
        "- **Positive: 34.68%**\n",
        "\n",
        "There's a heavy skew towards neutral sentiment, however, the positive tweets greatly outnumber the negative tweets. It is safe to say that overall, the NFL fanbase's sentiment towards this uncommon draft structure was positive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEfuzHAoJ5e2",
        "colab_type": "text"
      },
      "source": [
        "### 5.4. Analysis by Team"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaVPijaUJ5e2",
        "colab_type": "text"
      },
      "source": [
        "While utilizing only the teams hashtags contained in this dataset may not be the most ideal way to view each individual teams fans' sentiment, I'm curious to see the results in ours. I will be viewing sentiment towards the Green Bay Packers, Arizona Cardinals, and Kansas City Chiefs. The Packers had the worst draft in terms of consensus by analysts around the industry, while the Cardinals had one of the better drafts. The Chiefs are the returning superbowl champions, and have a relatively neutral consensus draft grade amongst analysts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQRokaHUJ5e2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Finding distinct values in hashtag column (returns a large amount)\n",
        "# hashtags = [i.hashtags for i in data.select('hashtags').distinct().collect()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trrL9IIDJ5e4",
        "colab_type": "text"
      },
      "source": [
        "#### 5.4.1. Green Bay Packers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7zi2gKbJ5e4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "4ca88d1e-23ef-4040-d584-01117d817b84"
      },
      "source": [
        "# Packers sentiment\n",
        "data.filter(data.hashtags.contains('Packers')).groupBy(\"sentiment\").count().orderBy(\"sentiment\").show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+-----+\n",
            "|sentiment|count|\n",
            "+---------+-----+\n",
            "| negative|    4|\n",
            "|  neutral|    8|\n",
            "| positive|    4|\n",
            "+---------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N82SHB8_J5e6",
        "colab_type": "text"
      },
      "source": [
        "#### 5.4.2. Arizona Cardinals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9F1oOHV5J5e6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "16c2b70f-3a1b-4428-d557-4ea01ed0ec8a"
      },
      "source": [
        "# Cardinals sentiment\n",
        "data.filter(data.hashtags.contains('Cardinals')).groupBy(\"sentiment\").count().orderBy(\"sentiment\").show()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+-----+\n",
            "|sentiment|count|\n",
            "+---------+-----+\n",
            "|  neutral|    7|\n",
            "| positive|    2|\n",
            "+---------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA1LWYUlJ5e8",
        "colab_type": "text"
      },
      "source": [
        "#### 5.4.3. Kansas City Chiefs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sCu9ibAJ5e9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "e64199c9-e6db-415e-e9f6-110b56ffa8e8"
      },
      "source": [
        "# Chiefs sentiment\n",
        "data.filter(data.hashtags.contains('Chiefs')).groupBy(\"sentiment\").count().orderBy(\"sentiment\").show()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+-----+\n",
            "|sentiment|count|\n",
            "+---------+-----+\n",
            "| negative|    2|\n",
            "|  neutral|    4|\n",
            "| positive|    4|\n",
            "+---------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH5zbXexJ5e-",
        "colab_type": "text"
      },
      "source": [
        "Again, the analysis suffers from a small sample size. An analysis for each team would likely be stronger if you scraped only keywords or hashtags that are team-specific."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DWECVbIJ5e_",
        "colab_type": "text"
      },
      "source": [
        "# 6. Naive Bayes Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8wCvRLnJ5e_",
        "colab_type": "text"
      },
      "source": [
        "Naive Bayes classifiers are a collection of classification algorithms based on **Bayes' Theorem**. This is a family of algorithms where every pair of features being classified is independent of each other. PySpark MLlib supports multinomial naive Bayes and Bernoulli naive Bayes. These models are typically used for document classification.\n",
        "\n",
        "In probability theory and statistics, **Bayes' Theorem** decribes the probability of an event based on prior knowledge of conditions that might be related to the event. Bayes' Theorem is mathematically stated as the equation below. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ohY6Q0tJ5fA",
        "colab_type": "text"
      },
      "source": [
        "![bayes.jpg](attachment:bayes.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4Tyvh6lJ5fA",
        "colab_type": "text"
      },
      "source": [
        "Where A and B are events and P(B) ? 0.\n",
        "- Trying to find probability of event A, given the event B is true. Event B is also termed as evidence.\n",
        "- P(A) is the prior probability of A. The evidence is an attribute value of an unknown instance(here, it is event B).\n",
        "- P(A|B) is a probability of event after evidence is seen of B."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_UpdXI2J5fA",
        "colab_type": "text"
      },
      "source": [
        "### 6.1. Create DataFrame for Naive Bayes Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJnOp2jQPvV6",
        "colab_type": "text"
      },
      "source": [
        "Due to some Java Heap Memory error complications, I will be performing this on a subset of the overall data (1000 rows)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jCDwNQTSMFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweetsubdf = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('NFLDraftTweets_small.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfGrGMOdSRDy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "fcc89dd4-4d08-45d8-d40b-d7700d871b2c"
      },
      "source": [
        "# Create newtweetdf with text and hashtag columns\n",
        "NBdf = tweetsubdf.drop('username', 'acctdesc', 'location', 'following', 'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts', 'retweetcount')\n",
        "\n",
        "# Drop duplicate rows\n",
        "NBdf = NBdf.dropDuplicates()\n",
        "\n",
        "# Drop rows with only null values\n",
        "NBdf = NBdf.dropna(how='all')\n",
        "\n",
        "# Remove non-ASCII characters\n",
        "NBdf = NBdf.withColumn('text_non_asci',strip_non_ascii_udf(NBdf['text']))\n",
        "\n",
        "# Fix abbreviations\n",
        "NBdf = NBdf.withColumn('fixed_abbrev',fix_abbreviation_udf(NBdf['text_non_asci']))\n",
        "\n",
        "#Get raw columns\n",
        "raw_cols = NBdf.columns\n",
        "\n",
        "# Remove stopwords\n",
        "NBdf = NBdf.select(raw_cols).withColumn(\"stopword_text\", remove_stops_udf(NBdf[\"fixed_abbrev\"]))\n",
        "\n",
        "# Remove irrelevant features\n",
        "NBdf = NBdf.select(raw_cols+[\"stopword_text\"]).withColumn(\"rem_feat_text\", remove_features_udf(NBdf[\"stopword_text\"]))\n",
        "\n",
        "# Lemmatization\n",
        "NBdf = NBdf.select(raw_cols+[\"rem_feat_text\"]).withColumn(\"cleaned_text\", lemmatize_udf(NBdf[\"rem_feat_text\"]))\n",
        "\n",
        "# DF for Sentiment analysis\n",
        "NBdf = NBdf.select('cleaned_text','hashtags')\n",
        "NBdf.show(1)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+\n",
            "|        cleaned_text|hashtags|\n",
            "+--------------------+--------+\n",
            "|post nfldraft rav...|    null|\n",
            "+--------------------+--------+\n",
            "only showing top 1 row\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-phRk9BUkP-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "03660ba2-f1d8-4c57-a0bc-d11ba720e78d"
      },
      "source": [
        "# Sentiment analysis on 1000 tweet dataset\n",
        "NBdf = NBdf.withColumn(\"sentiment_score\", sentiment_analysis_udf(NBdf['cleaned_text']))\n",
        "NBdf.show(1)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+---------------+\n",
            "|        cleaned_text|hashtags|sentiment_score|\n",
            "+--------------------+--------+---------------+\n",
            "|post nfldraft rav...|    null|            0.0|\n",
            "+--------------------+--------+---------------+\n",
            "only showing top 1 row\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmI0N2YBJ5fB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "c0f314f8-b3c9-454c-85b9-63704a113531"
      },
      "source": [
        "# Create DataFrame for Naive Bayes\n",
        "NBdf = NBdf.selectExpr(\"cleaned_text as text\", \"sentiment_score as label\")\n",
        "NBdf.show(1)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----+\n",
            "|                text|label|\n",
            "+--------------------+-----+\n",
            "|post nfldraft rav...|  0.0|\n",
            "+--------------------+-----+\n",
            "only showing top 1 row\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acLuRY8ZJ5fD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2252bee8-003a-4b13-cdad-eed2c3892c76"
      },
      "source": [
        "# Add unique ID\n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "\n",
        "NBdf = NBdf.withColumn(\"uid\", monotonically_increasing_id())\n",
        "NBdf = NBdf.select('uid', 'text', 'label')\n",
        "NBdf.show(5)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+--------------------+-----+\n",
            "|        uid|                text|label|\n",
            "+-----------+--------------------+-----+\n",
            "| 8589934592|post nfldraft rav...|  0.0|\n",
            "| 8589934593|check ezekiel ell...|  0.0|\n",
            "|17179869184|          tiger king|  0.0|\n",
            "|17179869185|cowboy nfldraft g...|  0.2|\n",
            "|17179869186|want make life ea...|  0.0|\n",
            "+-----------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONRkr-5xJ5fE",
        "colab_type": "text"
      },
      "source": [
        "### 6.2. Splitting Data Into Training and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvuGd9SOJ5fF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the data into training and test sets (60% / 40% split)\n",
        "(trainingData, testData) = NBdf.randomSplit([0.6, 0.4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ukttVxcJ5fG",
        "colab_type": "text"
      },
      "source": [
        "### 6.3. Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glGpiftWJ5fH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import PySpark Machine Learning Packages\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder\n",
        "from pyspark.ml.tuning import CrossValidator\n",
        "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
        "from pyspark.ml.feature import CountVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "4J98V0rxJ5fJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and nb.\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"rawFeatures\")\n",
        "\n",
        "# vectorizer = CountVectorizer(inputCol= \"words\", outputCol=\"rawFeatures\")\n",
        "idf = IDF(minDocFreq=3, inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "\n",
        "# Naive Bayes model\n",
        "nb = NaiveBayes()\n",
        "\n",
        "# Pipeline Architecture\n",
        "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, nb])\n",
        "\n",
        "# Train model.  This also runs the indexers.\n",
        "model = pipeline.fit(trainingData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAK4RqwIJ5fL",
        "colab_type": "text"
      },
      "source": [
        "### 6.4. Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_a_KlTwJ5fL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "3764f3ff-8293-49d7-b00a-4f549af47b40"
      },
      "source": [
        "# Make predictions\n",
        "predictions = model.transform(testData)\n",
        "\n",
        "# Select example rows to display.\n",
        "predictions.select(\"text\", \"label\", \"prediction\").show(5)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----+----------+\n",
            "|                text|label|prediction|\n",
            "+--------------------+-----+----------+\n",
            "|post nfldraft rav...|  0.0|      12.0|\n",
            "|          tiger king|  0.0|      12.0|\n",
            "|cowboy nfldraft g...|  0.2|      12.0|\n",
            "|total sec nfldraf...|-0.05|      12.0|\n",
            "|congrats caricspo...|0.275|      12.0|\n",
            "+--------------------+-----+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAGH1xW1J5fN",
        "colab_type": "text"
      },
      "source": [
        "### 6.5. Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4hJV20OJ5fN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "efa3208d-50f9-44b6-9981-1d189067ac6c"
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator.evaluate(predictions)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOSJ9IJPWYR-",
        "colab_type": "text"
      },
      "source": [
        "The Naive Bayes model strangely returned an accuracy of 0%. Let's try a different model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l09mceRZJ5fO",
        "colab_type": "text"
      },
      "source": [
        "# 7. HashingTF + IDF + Logistic Regression Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuoFB2-9WgDM",
        "colab_type": "text"
      },
      "source": [
        "Here, I will try to implement a TFIDF model with logistic regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0m3z_xmJ5fO",
        "colab_type": "text"
      },
      "source": [
        "### 7.1. Creating DataFrame for Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAMTSoAjJ5fP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "0514b751-e42e-40d9-eea2-d3af3aa2d4b0"
      },
      "source": [
        "modelData = NBdf.selectExpr(\"text as text\", \"label as target\")\n",
        "modelData.show(1)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+------+\n",
            "|                text|target|\n",
            "+--------------------+------+\n",
            "|post nfldraft rav...|   0.0|\n",
            "+--------------------+------+\n",
            "only showing top 1 row\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Mp-s0E8J5fQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "35d51ca8-dc86-495a-cf96-89cc0b084387"
      },
      "source": [
        "# Add unique ID\n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "\n",
        "modelData = modelData.withColumn(\"uid\", monotonically_increasing_id())\n",
        "modelData = modelData.select('uid', 'text', 'target')\n",
        "modelData.show(1)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+--------------------+------+\n",
            "|       uid|                text|target|\n",
            "+----------+--------------------+------+\n",
            "|8589934592|post nfldraft rav...|   0.0|\n",
            "+----------+--------------------+------+\n",
            "only showing top 1 row\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFAeWGhAJ5fR",
        "colab_type": "text"
      },
      "source": [
        "### 7.2. Splitting Data Into Training and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zps-D0nAJ5fS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train_set, val_set, test_set) = modelData.randomSplit([0.98, 0.01, 0.01], seed = 2000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B21jN7BSJ5fT",
        "colab_type": "text"
      },
      "source": [
        "### 7.3. Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ey9dTjsJ5fT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddDbOxhJJ5fV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "643cae4e-212b-4283-dd44-1414565e5f68"
      },
      "source": [
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
        "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=3) #minDocFreq: remove sparse terms\n",
        "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
        "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx])\n",
        "\n",
        "pipelineFit = pipeline.fit(train_set)\n",
        "train_df = pipelineFit.transform(train_set)\n",
        "val_df = pipelineFit.transform(val_set)\n",
        "train_df.show(5)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
            "|        uid|                text|target|               words|                  tf|            features|label|\n",
            "+-----------+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
            "| 8589934592|post nfldraft rav...|   0.0|[post, nfldraft, ...|(65536,[12929,232...|(65536,[12929,232...|  0.0|\n",
            "| 8589934593|check ezekiel ell...|   0.0|[check, ezekiel, ...|(65536,[235,5630,...|(65536,[235,5630,...|  0.0|\n",
            "|17179869184|          tiger king|   0.0|       [tiger, king]|(65536,[10302,326...|(65536,[10302,326...|  0.0|\n",
            "|17179869185|cowboy nfldraft g...|   0.2|[cowboy, nfldraft...|(65536,[4211,8062...|(65536,[4211,8062...|  6.0|\n",
            "|17179869186|want make life ea...|   0.0|[want, make, life...|(65536,[4882,6052...|(65536,[4882,6052...|  0.0|\n",
            "+-----------+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3KLznItJ5fW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "faf16b4b-c6a1-48c8-a856-545f0a25bcfe"
      },
      "source": [
        "lr = LogisticRegression(maxIter=100)\n",
        "lrModel = lr.fit(train_df)\n",
        "predictions = lrModel.transform(val_df)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-35045f9491d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxIter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlrModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlrModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2blVHT9hXrzV",
        "colab_type": "text"
      },
      "source": [
        "### 7.4. Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikCDH9vwJ5fZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "650f4626-ed6b-4deb-96a8-9685c11a2094"
      },
      "source": [
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
        "evaluator.evaluate(predictions)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-923c75f2142d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBinaryClassificationEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawPredictionCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rawPrediction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \"\"\"\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1491.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 65 in stage 139.0 failed 1 times, most recent failure: Lost task 65.0 in stage 139.0 (TID 6797, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:309)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:151)\n\tat org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:62)\n\tat org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:61)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.OrderedRDDFunctions.sortByKey(OrderedRDDFunctions.scala:61)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4$lzycompute(BinaryClassificationMetrics.scala:155)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4(BinaryClassificationMetrics.scala:146)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions$lzycompute(BinaryClassificationMetrics.scala:148)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions(BinaryClassificationMetrics.scala:148)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.createCurve(BinaryClassificationMetrics.scala:226)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.roc(BinaryClassificationMetrics.scala:86)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.areaUnderROC(BinaryClassificationMetrics.scala:97)\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator.evaluate(BinaryClassificationEvaluator.scala:87)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwKGABfZXaft",
        "colab_type": "text"
      },
      "source": [
        "For binary classification, Spark doesn't support accuracy as a metric. I can calculate accuracy by counting the number of predictions matching the label and dividing it by the total entries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_M-RIqTJ5fe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
        "accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ8SR7kTJ5fg",
        "colab_type": "text"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyoJEKKSJ5fg",
        "colab_type": "text"
      },
      "source": [
        "**Content Sources:**\n",
        "- https://www.techopedia.com/definition/29019/cloud-server\n",
        "- https://www.pythonanywhere.com\n",
        "- http://docs.tweepy.org/en/latest/\n",
        "- https://www.geeksforgeeks.org/twitter-sentiment-analysis-using-python/\n",
        "- http://adilmoujahid.com/posts/2014/07/twitter-analytics/\n",
        "- https://stackoverflow.com/questions/24214189/how-can-i-get-tweets-older-than-a-week-using-tweepy-or-other-python-libraries\n",
        "- http://spark.apache.org/docs/latest/#launching-on-a-cluster\n",
        "- https://docs.databricks.com/data/data-sources/read-csv.html\n",
        "- https://towardsdatascience.com/creating-the-twitter-sentiment-analysis-program-in-python-with-naive-bayes-classification-672e5589a7ed\n",
        "- https://towardsdatascience.com/sentiment-analysis-with-pyspark-bc8e83f80c35\n",
        "- https://medium.com/@leowgriffin/scraping-tweets-with-tweepy-python-59413046e788\n",
        "- https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html\n",
        "- https://runawayhorse001.github.io/LearningApacheSpark/textmining.html\n",
        "- https://www.nltk.org/\n",
        "- https://classes.ischool.syr.edu/ist718/content/unit09/lab-sentiment_analysis/\n",
        "- http://www.datasciencemadesimple.com/subset-or-filter-data-with-multiple-conditions-in-pyspark/\n",
        "- https://spark.apache.org/docs/2.4.5/\n",
        "- http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.NaiveBayes\n",
        "- Prior notebooks created for homeworks, lecture notes, other classes' materials\n",
        "\n",
        "**Photo sources:**\n",
        "- http://spark.apache.org/\n",
        "- https://realpython.com/twitter-bot-python-tweepy/\n",
        "- https://oh42fifty.org/wp-content/uploads/2020/05/Winners_and_Losers_of_the_2020_NFL_Draft.jpg\n",
        "- https://runawayhorse001.github.io/LearningApacheSpark/_images/sentiment_analysis_pipeline.png\n",
        "- https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-7777aa719ea14857115695676adc0914_l3.svg"
      ]
    }
  ]
}