{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mallinson_T_CIS731_FinalProject.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wUZGqoso8ps",
        "colab_type": "text"
      },
      "source": [
        "<h1 style=\"text-align: center;\"><span style=\"color: #000000;\"> CIS 731 Final Project: Sentiment Analysis Using Pyspark on Fan's Tweets Following the NFL Draft\n",
        "</span></h1>\n",
        "<h3 style=\"text-align: center;\"><span style=\"color: #000000;\"> Created by: Thomas Mallinson\n",
        "</span></h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMQ6BEPwmf27",
        "colab_type": "text"
      },
      "source": [
        "![](https://drive.google.com/uc?id=1jI9JZLou2js0bVN99G99531aGhhigWok)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlnjBRQwpCTq",
        "colab_type": "text"
      },
      "source": [
        "# 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvLqOKt_pC52",
        "colab_type": "text"
      },
      "source": [
        "### 1.1. Procedure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3irlZ1zppDKq",
        "colab_type": "text"
      },
      "source": [
        "This project will descriptive analytics and content analytics on keywords pertaining to Tweets regarding the 2020 NFL Draft. For the first time ever, the draft was held virtually due to the COVID-19 Pandemic. I want to see how fans reacted to the new structure. I will scrape tweets using Tweepy and collect them using the cloud server PythonAnywhere. Tweet data will be analyzed by applying descriptive, content, and network analytics techniques using Python and PySpark. I will use a Naive Bayes classification algorithm to classify the text and sentiment analysis to record the feeling towards the results of the draft."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxCBpknEpDPB",
        "colab_type": "text"
      },
      "source": [
        "### 1.2. Procedure Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u18m34vupDZ4",
        "colab_type": "text"
      },
      "source": [
        "**Cloud Server/Cloud Computing:** A cloud server is a logical server that is built, hosted and delivered through a cloud computing platform over the Internet. Cloud servers possess and exhibit similar capabilities and functionality to a typical server but are accessed remotely from a cloud service provider. PythonAnywhere is an online integrated development environment and web hosting service based on the Python programming language.\n",
        "\n",
        "**PySpark:** Apache Spark is an analytics engine and parallel computation framework with Scala, Python, Java, and R interfaces. Spark can load data directly from disk, memory and other data storage technologies such as Amazon S3, Hadoop Distributed File System (HDFS), Cassandra and others. PySpark is collaboration of Apache Spark and Python, a general-purpose, high-level programming language. \n",
        "\n",
        "**Tweepy:** Tweepy is a Python package that provides a convenient way to use the Twitter API. The Twitter API gives developers access to most of Twitter's functionality, such as reading and writing information related to tweets, users, and trends."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GyDDrEApDch",
        "colab_type": "text"
      },
      "source": [
        "### 1.3. Sentiment Analysis Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJrYA6KMpEAI",
        "colab_type": "text"
      },
      "source": [
        "![](https://drive.google.com/uc?id=1nq--wHyrmW9eRDEqSwfzlan92tVUGY20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1j3j33hopECq",
        "colab_type": "text"
      },
      "source": [
        "# 2. Twitter Data Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myAI6lFgpEed",
        "colab_type": "text"
      },
      "source": [
        "### 2.1. Streaming Twitter Data and Saving in .csv Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1ywTxsupEnc",
        "colab_type": "text"
      },
      "source": [
        "The following code was ran in a PythonAnywhere Cloud Server for hashtags, or keywords, associated with the NFL Draft. In order to access Twitter's API, I created an account on Algorithmia.com and then a Twitter developer account and went through the necessary steps to obtain access. I used multiple API calls in order to collect 50,000 total tweets for a span of five days, ranging from the first round of the draft (Day One) to two days following the third and final draft day. Due to the quantity of tweets utilizing the particular hashtags I used as keys, I had to scrape in separate .py files and then combine them before preprocessing here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7P6qeEopExd",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "# Call the function scrapetweets\n",
        "scrapetweets(search_words, date_since, date_until, numTweets, numRuns) # Twitter credentials\n",
        "consumer_key = 'ZySgTb5HKxTZkqKzurkVW5ONG'\n",
        "consumer_secret = 'eMVDQwD25CM9EkE4ly9MkLjAvxUvKtk3zku4hZvuuYjp26XNwo'\n",
        "access_key = '1176869078738378753-VD2OWybAZHmXt0btIPHMn9ClB25ObD'\n",
        "access_secret = '1WxN4OoRvVloYnGGhZVpIzGvw9StYydLeBfliX6L9IXML'\n",
        "\n",
        "# Pass your twitter credentials to tweepy via its OAuthHandler\n",
        "auth = OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_key, access_secret)\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "# Calls API every 15 minutes to prevent overcalling\n",
        "\n",
        "def scrapetweets(search_words, date_since, date_until, numTweets, numRuns):\n",
        "\n",
        "    ## Arguments:\n",
        "    # search_words -> define a string of keywords for this function to extract\n",
        "    # date_since -> define a date from which to start extracting the tweets \n",
        "    # date_until -> define a date from which to end extracting the tweets \n",
        "    # numTweets -> number of tweets to extract per run\n",
        "    # numRun -> number of runs to perform in this program - API calls are limited to once every 15 mins, so each run will be 15 mins apart.\n",
        "    \n",
        "    # Define a pandas dataframe to store the date:\n",
        "    db_tweets = pd.DataFrame(columns = ['username', 'acctdesc', 'location', 'following',\n",
        "                                        'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts',\n",
        "                                        'retweetcount', 'text', 'hashtags']\n",
        "                                )\n",
        "    # Define a for-loop to generate tweets at regular intervals\n",
        "    for i in range(0, numRuns):\n",
        "        # We will time how long it takes to scrape tweets for each run:\n",
        "        start_run = time.time()\n",
        "        \n",
        "        # Collect tweets using the Cursor object\n",
        "        # .Cursor() returns an object that you can iterate or loop over to access the data collected.\n",
        "        # Each item in the iterator has various attributes that you can access to get information about each tweet\n",
        "        tweets = tweepy.Cursor(api.search, q=search_words, lang=\"en\", since=date_since, until=date_until, tweet_mode='extended').items(numTweets)\n",
        "\n",
        "        # Store these tweets into a python list\n",
        "        tweet_list = [tweet for tweet in tweets]\n",
        "\n",
        "        # Begin scraping the tweets individually:\n",
        "        noTweets = 0\n",
        "\n",
        "        for tweet in tweet_list:\n",
        "\n",
        "            # Pull the values\n",
        "            username = tweet.user.screen_name\n",
        "            acctdesc = tweet.user.description\n",
        "            location = tweet.user.location\n",
        "            following = tweet.user.friends_count\n",
        "            followers = tweet.user.followers_count\n",
        "            totaltweets = tweet.user.statuses_count\n",
        "            usercreatedts = tweet.user.created_at\n",
        "            tweetcreatedts = tweet.created_at\n",
        "            retweetcount = tweet.retweet_count\n",
        "            hashtags = tweet.entities['hashtags']\n",
        "\n",
        "            try:\n",
        "                text = tweet.retweeted_status.full_text\n",
        "            except AttributeError:  # Not a Retweet\n",
        "                text = tweet.full_text\n",
        "\n",
        "            # Add the 11 variables to the empty list - ith_tweet:\n",
        "            ith_tweet = [username, acctdesc, location, following, followers, totaltweets,\n",
        "                         usercreatedts, tweetcreatedts, retweetcount, text, hashtags]\n",
        "\n",
        "            # Append to dataframe - db_tweets\n",
        "            db_tweets.loc[len(db_tweets)] = ith_tweet\n",
        "\n",
        "            # increase counter - noTweets  \n",
        "            noTweets += 1\n",
        "        \n",
        "        # Run ended:\n",
        "        end_run = time.time()\n",
        "        duration_run = round(end_run-start_run, 2)\n",
        "        \n",
        "        print('no. of tweets scraped for run {} is {}'.format(i, noTweets))\n",
        "        print('time take for {} run to complete is {}'.format(i, duration_run))\n",
        "        \n",
        "        time.sleep(900) #15 minute sleep time\n",
        "\n",
        "        \n",
        "    # Once all runs have completed, save them to a single csv file:    \n",
        "    # Obtain timestamp in a readable format:\n",
        "    from datetime import datetime\n",
        "    to_csv_timestamp = datetime.today().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "    # Define working path and filename\n",
        "    path = os.getcwd()\n",
        "    filename = path + '/data/' + to_csv_timestamp + 'nfldraftsample.csv'\n",
        "\n",
        "    # Store dataframe in csv with creation date timestamp\n",
        "    db_tweets.to_csv(filename, index = False)\n",
        "    \n",
        "    print('Scraping has completed!')\n",
        "\n",
        "# Initialise these variables:\n",
        "search_words = \"#nfldraft OR #nfldraft2020 OR #nfldraftday OR #draft OR #draft2020\"\n",
        "date_since = \"2020-4-23\"\n",
        "date_until = \"2020-4-27\"\n",
        "numTweets = 2500\n",
        "numRuns = 20\n",
        "# Call the function scrapetweets\n",
        "scrapetweets(search_words, date_since, date_until, numTweets, numRuns)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY8n69YWpFvy",
        "colab_type": "text"
      },
      "source": [
        "### 2.2. Key Attributes in Tweet .csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtQvdwjxpF33",
        "colab_type": "text"
      },
      "source": [
        "**The following information was collected:**\n",
        "- user.screen_name - twitter handle\n",
        "- user.description - description of account\n",
        "- user.location - where is he tweeting from\n",
        "- user.friends_count - no. of other users that user is following (following)\n",
        "- user.followers_count - no. of other users who are following this user (followers)\n",
        "- user.statuses_count - total tweets by user\n",
        "- user.created_at - when the user account was created\n",
        "- created_at - when the tweet was created\n",
        "- retweet_count - no. of retweets\n",
        "- (deprecated) user.favourites_count - probably total no. of tweets that is favourited by user\n",
        "- retweeted_status.full_text - full text of the tweet\n",
        "- tweet.entities['hashtags'] - hashtags in the tweet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DkttcnIpGSG",
        "colab_type": "text"
      },
      "source": [
        "# 3. Loading Tweet Data Into PySpark DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-5sYWqHeqXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar -xvf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaMagU9Ceudc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "\n",
        "def create_spark_context():\n",
        "  return SparkContext.getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm91V0tuewCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext, SparkSession\n",
        "\n",
        "# Setting spark.driver.memory due to issues faced with pyspark.ml processing\n",
        "SparkContext.setSystemProperty('spark.driver.memory', '16g')\n",
        "\n",
        "# Create Spark Contexts\n",
        "sc = pyspark.SparkContext()\n",
        "sc.getConf()\n",
        "conf = pyspark.SparkConf().setAll([('spark.executor.memory', '16g'), ('spark.executor.cores', '3'), ('spark.cores.max', '3'), ('spark.driver.memory','8g'), (\"spark.memory.offHeap.size\",\"16g\"),('spark.memory.offHeap.enabled','true')])\n",
        "sc.stop()\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "sqlContext = SQLContext(sc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2iZCjJ0ewr9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "86162856-4679-4950-b66e-c2b8e0d02323"
      },
      "source": [
        "sc._conf.getAll()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spark.driver.port', '39817'),\n",
              " ('spark.executor.id', 'driver'),\n",
              " ('spark.memory.offHeap.enabled', 'true'),\n",
              " ('spark.app.name', 'pyspark-shell'),\n",
              " ('spark.cores.max', '3'),\n",
              " ('spark.executor.memory', '16g'),\n",
              " ('spark.driver.host', '3e017c49d3df'),\n",
              " ('spark.rdd.compress', 'True'),\n",
              " ('spark.driver.memory', '8g'),\n",
              " ('spark.serializer.objectStreamReset', '100'),\n",
              " ('spark.master', 'local[*]'),\n",
              " ('spark.executor.cores', '3'),\n",
              " ('spark.submit.deployMode', 'client'),\n",
              " ('spark.ui.showConsoleProgress', 'true'),\n",
              " ('spark.memory.offHeap.size', '16g'),\n",
              " ('spark.app.id', 'local-1589649321963')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNcTh-Z8fHvq",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "0ddbeca8-7e1c-4f68-f94c-b21e35483251"
      },
      "source": [
        "# Import NFL Draft Tweets data\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2ea9bf72-5b4d-4bad-abf2-345e12c3a79c\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-2ea9bf72-5b4d-4bad-abf2-345e12c3a79c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving NFLDraftTweets.csv to NFLDraftTweets.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wDbsuF4eyk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweetdf = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('NFLDraftTweets.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_WR8Wmvgfky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head -1000 NFLDraftTweets.csv >> NFLDraftTweets_small.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snL8qEu4giDu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ff87f4f-ad47-4e25-acf1-e944628faecf"
      },
      "source": [
        "type(tweetdf)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z031N55q0YO",
        "colab_type": "text"
      },
      "source": [
        "Above, we can see that this is in a PySpark DataFrame data structure. DataFrames in Spark are immutable, distributed, and designed to process structured data. To help Apache Spark understand the schema, the data is organized under named columns. This helps Spark optimize execution plans and handle pedabytes of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETxoU0IEgiKP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "1ec6dee0-adee-4533-cf75-3255bd843ee1"
      },
      "source": [
        "#Display first five rows of data\n",
        "tweetdf.show(5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+---------------+---------+---------+---------------+----------------+---------------+--------------------+--------------------+--------------------+\n",
            "|            username|            acctdesc|       location|following|followers|    totaltweets|   usercreatedts| tweetcreatedts|        retweetcount|                text|            hashtags|\n",
            "+--------------------+--------------------+---------------+---------+---------+---------------+----------------+---------------+--------------------+--------------------+--------------------+\n",
            "|     Spectpooheagles|Optimuspooh, Mixe...|Philadelphia,pa|     1948|     1337|         206944|  4/26/2011 2:05|4/27/2020 23:59|                  52|#NFL #NFLDraft Te...|[{'text': 'NFL', ...|\n",
            "|         sammydabber|st rose class of ...|           null|     null|     null|           null|            null|           null|                null|                null|                null|\n",
            "|I like carmella m...|       United States|            470|       60|    25261|3/14/2019 20:12| 4/27/2020 23:59|            112|Moments after bei...|                null|                null|\n",
            "|#GoBills | #NFLDr...|[{'text': 'GoBill...|           null|     null|     null|           null|            null|           null|                null|                null|                null|\n",
            "|             LDW1884|                null|           null|     4394|     1007|          56953|11/29/2014 18:14|4/27/2020 23:59|                 642|The first wide re...|                null|\n",
            "+--------------------+--------------------+---------------+---------+---------+---------------+----------------+---------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UodfATKgiMn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "e8a409ad-7a53-41c3-8ec1-ec0b93dbf349"
      },
      "source": [
        "tweetdf.printSchema()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- username: string (nullable = true)\n",
            " |-- acctdesc: string (nullable = true)\n",
            " |-- location: string (nullable = true)\n",
            " |-- following: string (nullable = true)\n",
            " |-- followers: string (nullable = true)\n",
            " |-- totaltweets: string (nullable = true)\n",
            " |-- usercreatedts: string (nullable = true)\n",
            " |-- tweetcreatedts: string (nullable = true)\n",
            " |-- retweetcount: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- hashtags: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntFrdn3Nq4Qa",
        "colab_type": "text"
      },
      "source": [
        "# 4. Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JKSGnLWq6Ht",
        "colab_type": "text"
      },
      "source": [
        "### 4.1. Creating New DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkIyU7Izq749",
        "colab_type": "text"
      },
      "source": [
        "Before applying performing sentiment analysis or applying a classification algorithm, we need to preprocess the data. First, I will create a new dataframe containing only the important columns we will be looking at in this project (text and hashtags). Then, I will perform custom transformations by creating Spark's User-Defined Functions (UDFs) for different common preprocessing techniques. I will be using the Natural Language Toolkit (NLTK) Python Package to do some of the preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3enxbgv_giO9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "10f1b9ad-9332-427a-d456-0606294683a1"
      },
      "source": [
        "#Create newtweetdf with text and hashtag columns\n",
        "newtweetdf = tweetdf.drop('username', 'acctdesc', 'location', 'following', 'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts', 'retweetcount')\n",
        "\n",
        "#Drop duplicate rows\n",
        "newtweetdf = newtweetdf.dropDuplicates()\n",
        "\n",
        "#Drop rows with only null values\n",
        "newtweetdf = newtweetdf.dropna(how='all')\n",
        "newtweetdf.show(5)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+\n",
            "|                text|hashtags|\n",
            "+--------------------+--------+\n",
            "|How did you @Bill...|      []|\n",
            "|RD 2 | Pick 41 - ...|    null|\n",
            "|Nice local haul i...|    null|\n",
            "|      Dear @Raiders,|    null|\n",
            "|Garrett Taylor (@...|    null|\n",
            "+--------------------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfYjZLaRq-m7",
        "colab_type": "text"
      },
      "source": [
        "### 4.2. Remove Non-ASCII Characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhlyG-O2giRN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "e2daec23-50c2-4c9b-8104-bc1d15342169"
      },
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "set(stopwords.words(\"english\"))\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk import pos_tag\n",
        "import string\n",
        "import re"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOKVYwJKgiT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def strip_non_ascii(data_str):\n",
        "    ''' Returns the string without non ASCII characters'''\n",
        "    stripped = (c for c in data_str if 0 < ord(c) < 127)\n",
        "    return ''.join(stripped)\n",
        "\n",
        "# Setup PySpark UDF Function\n",
        "strip_non_ascii_udf = udf(strip_non_ascii, StringType())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1hnzMt8giWN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9f0561fa-0510-4150-8961-e3b04cc4ea32"
      },
      "source": [
        "nonascii_df = newtweetdf.withColumn('text_non_asci',strip_non_ascii_udf(newtweetdf['text']))\n",
        "nonascii_df.show(5)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+--------------------+\n",
            "|                text|hashtags|       text_non_asci|\n",
            "+--------------------+--------+--------------------+\n",
            "|How did you @Bill...|      []|How did you @Bill...|\n",
            "|RD 2 | Pick 41 - ...|    null|RD 2 | Pick 41 - ...|\n",
            "|Nice local haul i...|    null|Nice local haul i...|\n",
            "|      Dear @Raiders,|    null|      Dear @Raiders,|\n",
            "|Garrett Taylor (@...|    null|Garrett Taylor (@...|\n",
            "+--------------------+--------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ-VkhqfrD4C",
        "colab_type": "text"
      },
      "source": [
        "### 4.3. Fix Abbreviations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPbxoYR6giYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fix_abbreviation(data_str):\n",
        "    data_str = data_str.lower()\n",
        "    data_str = re.sub(r'\\bthats\\b', 'that is', data_str)\n",
        "    data_str = re.sub(r'\\bive\\b', 'i have', data_str)\n",
        "    data_str = re.sub(r'\\bim\\b', 'i am', data_str)\n",
        "    data_str = re.sub(r'\\bya\\b', 'yeah', data_str)\n",
        "    data_str = re.sub(r'\\bcant\\b', 'can not', data_str)\n",
        "    data_str = re.sub(r'\\bdont\\b', 'do not', data_str)\n",
        "    data_str = re.sub(r'\\bwont\\b', 'will not', data_str)\n",
        "    data_str = re.sub(r'\\bid\\b', 'i would', data_str)\n",
        "    data_str = re.sub(r'\\bwth\\b', 'what the hell', data_str)\n",
        "    data_str = re.sub(r'\\br\\b', 'are', data_str)\n",
        "    data_str = re.sub(r'\\bu\\b', 'you', data_str)\n",
        "    data_str = re.sub(r'\\bk\\b', 'OK', data_str)\n",
        "    data_str = re.sub(r'\\bsux\\b', 'sucks', data_str)\n",
        "    data_str = re.sub(r'\\bno+\\b', 'no', data_str)\n",
        "    data_str = re.sub(r'\\bcoo+\\b', 'cool', data_str)\n",
        "    data_str = re.sub(r'rt\\b', '', data_str)\n",
        "    data_str = data_str.strip()\n",
        "    return data_str\n",
        "\n",
        "# Setup PySpark UDF Function\n",
        "fix_abbreviation_udf = udf(fix_abbreviation, StringType())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlWZ-j4-giaz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ad60f0ee-8f79-422a-f9f4-e3c21cdd7054"
      },
      "source": [
        "fixabb_df = nonascii_df.withColumn('fixed_abbrev',fix_abbreviation_udf(nonascii_df['text_non_asci']))\n",
        "fixabb_df.show(5)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+--------------------+--------------------+\n",
            "|                text|hashtags|       text_non_asci|        fixed_abbrev|\n",
            "+--------------------+--------+--------------------+--------------------+\n",
            "|How did you @Bill...|      []|How did you @Bill...|how did you @bill...|\n",
            "|RD 2 | Pick 41 - ...|    null|RD 2 | Pick 41 - ...|rd 2 | pick 41 - ...|\n",
            "|Nice local haul i...|    null|Nice local haul i...|nice local haul i...|\n",
            "|      Dear @Raiders,|    null|      Dear @Raiders,|      dear @raiders,|\n",
            "|Garrett Taylor (@...|    null|Garrett Taylor (@...|garrett taylor (@...|\n",
            "+--------------------+--------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y49GfB6crGkC",
        "colab_type": "text"
      },
      "source": [
        "### 4.4. Remove Stop Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3Hm6iYJrIO_",
        "colab_type": "text"
      },
      "source": [
        "Stop words are some of the most common words in English like \"a\", \"the\", \"is\", etc. They are generally removed from text because they do not carry any sentiment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5WQwgrEgidH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_stops(data_str):\n",
        "    # expects a string\n",
        "    stops = stopwords.words()\n",
        "    list_pos = 0\n",
        "    cleaned_str = ''\n",
        "    text = data_str.split()\n",
        "    for word in text:\n",
        "        if word not in stops:\n",
        "            # rebuild cleaned_str\n",
        "            if list_pos == 0:\n",
        "                cleaned_str = word\n",
        "            else:\n",
        "                cleaned_str = cleaned_str + ' ' + word\n",
        "            list_pos += 1\n",
        "    return cleaned_str\n",
        "\n",
        "# Setup PySpark UDF function\n",
        "remove_stops_udf = udf(remove_stops, StringType())\n",
        "\n",
        "#Get raw columns\n",
        "raw_cols = fixabb_df.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX0ZR5dOg5RC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9ee32235-225b-4f32-dc8b-e05f934768a3"
      },
      "source": [
        "stopwordsdf = fixabb_df.select(raw_cols).withColumn(\"stopword_text\", remove_stops_udf(fixabb_df[\"fixed_abbrev\"]))\n",
        "stopwordsdf.show(5)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+--------------------+--------------------+--------------------+\n",
            "|                text|hashtags|       text_non_asci|        fixed_abbrev|       stopword_text|\n",
            "+--------------------+--------+--------------------+--------------------+--------------------+\n",
            "|How did you @Bill...|      []|How did you @Bill...|how did you @bill...|@billgates go dev...|\n",
            "|RD 2 | Pick 41 - ...|    null|RD 2 | Pick 41 - ...|rd 2 | pick 41 - ...|rd 2 | pick 41 - ...|\n",
            "|Nice local haul i...|    null|Nice local haul i...|nice local haul i...|nice local haul #...|\n",
            "|      Dear @Raiders,|    null|      Dear @Raiders,|      dear @raiders,|      dear @raiders,|\n",
            "|Garrett Taylor (@...|    null|Garrett Taylor (@...|garrett taylor (@...|garrett taylor (@...|\n",
            "+--------------------+--------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP4fKJNwrNxK",
        "colab_type": "text"
      },
      "source": [
        "### 4.5. Remove Irrelevant Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfO1HRxSrO_m",
        "colab_type": "text"
      },
      "source": [
        "Here, I'll remove various features from the such such as hyperlinks, mentions, short words, and punctuation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fbla7mjwg5Th",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_features(data_str):\n",
        "    # Compile regex\n",
        "    url_re = re.compile('https?://(www.)?\\w+\\.\\w+(/\\w+)*/?')\n",
        "    punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    num_re = re.compile('(\\\\d+)')\n",
        "    mention_re = re.compile('@(\\w+)')\n",
        "    alpha_num_re = re.compile(\"^[a-z0-9_.]+$\")\n",
        "    # Remove hyperlinks\n",
        "    data_str = url_re.sub(' ', data_str)\n",
        "    # Remove @mentions\n",
        "    data_str = mention_re.sub(' ', data_str)\n",
        "    # Remove puncuation\n",
        "    data_str = punc_re.sub(' ', data_str)\n",
        "    # Remove numeric 'words'\n",
        "    data_str = num_re.sub(' ', data_str)\n",
        "    # Remove non a-z 0-9 characters and words shorter than 3 characters\n",
        "    list_pos = 0\n",
        "    cleaned_str = ''\n",
        "    for word in data_str.split():\n",
        "        if list_pos == 0:\n",
        "            if alpha_num_re.match(word) and len(word) > 2:\n",
        "                cleaned_str = word\n",
        "            else:\n",
        "                cleaned_str = ' '\n",
        "        else:\n",
        "            if alpha_num_re.match(word) and len(word) > 2:\n",
        "                cleaned_str = cleaned_str + ' ' + word\n",
        "            else:\n",
        "                cleaned_str += ' '\n",
        "        list_pos += 1\n",
        "    return cleaned_str\n",
        "\n",
        "#Setup PySpark UDF Function\n",
        "remove_features_udf = udf(remove_features, StringType())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YiwnCGJg5Vr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "7c852e21-1f95-4f3f-810d-50afe8fa0834"
      },
      "source": [
        "rem_features_df = stopwordsdf.select(raw_cols+[\"stopword_text\"]).withColumn(\"rem_feat_text\", remove_features_udf(stopwordsdf[\"stopword_text\"]))\n",
        "rem_features_df.show(5)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|hashtags|       text_non_asci|        fixed_abbrev|       stopword_text|       rem_feat_text|\n",
            "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+\n",
            "|How did you @Bill...|      []|How did you @Bill...|how did you @bill...|@billgates go dev...|  developing code...|\n",
            "|RD 2 | Pick 41 - ...|    null|RD 2 | Pick 41 - ...|rd 2 | pick 41 - ...|rd 2 | pick 41 - ...|  pick jonathan t...|\n",
            "|Nice local haul i...|    null|Nice local haul i...|nice local haul i...|nice local haul #...|nice local haul n...|\n",
            "|      Dear @Raiders,|    null|      Dear @Raiders,|      dear @raiders,|      dear @raiders,|                dear|\n",
            "|Garrett Taylor (@...|    null|Garrett Taylor (@...|garrett taylor (@...|garrett taylor (@...|garrett taylor he...|\n",
            "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDfAZjt2rSOr",
        "colab_type": "text"
      },
      "source": [
        "### 4.6. Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGqcwg5LrT_h",
        "colab_type": "text"
      },
      "source": [
        "The goal of lemmmatization is to remove inflections and map a word to it's root form. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6D9ymDlg5YZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lemmatize(data_str):\n",
        "    # expects a string\n",
        "    list_pos = 0\n",
        "    cleaned_str = ''\n",
        "    lmtzr = WordNetLemmatizer()\n",
        "    text = data_str.split()\n",
        "    tagged_words = pos_tag(text)\n",
        "    for word in tagged_words:\n",
        "        if 'v' in word[1].lower():\n",
        "            lemma = lmtzr.lemmatize(word[0], pos='v')\n",
        "        else:\n",
        "            lemma = lmtzr.lemmatize(word[0], pos='n')\n",
        "        if list_pos == 0:\n",
        "            cleaned_str = lemma\n",
        "        else:\n",
        "            cleaned_str = cleaned_str + ' ' + lemma\n",
        "        list_pos += 1\n",
        "    return cleaned_str\n",
        "\n",
        "# Setup PySpark UDF Function\n",
        "lemmatize_udf = udf(lemmatize, StringType())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEJYy3ukg5as",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "5f4eb9be-563e-4f58-b4c9-75ab4901d425"
      },
      "source": [
        "lemm_df = rem_features_df.select(raw_cols+[\"rem_feat_text\"]).withColumn(\"cleaned_text\", lemmatize_udf(rem_features_df[\"rem_feat_text\"]))\n",
        "lemm_df.show(5)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|hashtags|       text_non_asci|        fixed_abbrev|       rem_feat_text|        cleaned_text|\n",
            "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+\n",
            "|How did you @Bill...|      []|How did you @Bill...|how did you @bill...|  developing code...|develop code comp...|\n",
            "|RD 2 | Pick 41 - ...|    null|RD 2 | Pick 41 - ...|rd 2 | pick 41 - ...|  pick jonathan t...|pick jonathan tay...|\n",
            "|Nice local haul i...|    null|Nice local haul i...|nice local haul i...|nice local haul n...|nice local haul n...|\n",
            "|      Dear @Raiders,|    null|      Dear @Raiders,|      dear @raiders,|                dear|                dear|\n",
            "|Garrett Taylor (@...|    null|Garrett Taylor (@...|garrett taylor (@...|garrett taylor he...|garrett taylor he...|\n",
            "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kEiEgyvrXCk",
        "colab_type": "text"
      },
      "source": [
        "### 4.5. Create Cleaned DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slz793wBg5c3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a4d30b3c-9a82-4c03-852a-93cfde939d76"
      },
      "source": [
        "data = lemm_df.select('cleaned_text','hashtags')\n",
        "data.show(5)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+\n",
            "|        cleaned_text|hashtags|\n",
            "+--------------------+--------+\n",
            "|develop code comp...|      []|\n",
            "|pick jonathan tay...|    null|\n",
            "|nice local haul n...|    null|\n",
            "|                dear|    null|\n",
            "|garrett taylor he...|    null|\n",
            "+--------------------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCpvjUFhhW1C",
        "colab_type": "text"
      },
      "source": [
        "# 5. Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HEkMLRMra98",
        "colab_type": "text"
      },
      "source": [
        "Sentiment Analysis is the process of 'computationally' determining whether a piece of writing is positive, negative, or neutral in order to try and determine the attitude of the writer. It's widely used by businesses for analyzing customer reviews, social media, and survey responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9O8oV22rc5H",
        "colab_type": "text"
      },
      "source": [
        "### 5.1. Creating the Sentiment Analysis Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjzSyA0ig5hw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.types import FloatType\n",
        "from textblob import TextBlob\n",
        "\n",
        "def sentiment_analysis(text):\n",
        "    return TextBlob(text).sentiment.polarity\n",
        "\n",
        "# Setup PySpark UDF Function\n",
        "sentiment_analysis_udf = udf(sentiment_analysis , FloatType())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJDpAgC-g5jh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "81854a1d-3e37-44b3-be0d-3466aba205f1"
      },
      "source": [
        "data = data.withColumn(\"sentiment_score\", sentiment_analysis_udf(data['cleaned_text']))\n",
        "data.show(5)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+---------------+\n",
            "|        cleaned_text|hashtags|sentiment_score|\n",
            "+--------------------+--------+---------------+\n",
            "|develop code comp...|      []|            0.0|\n",
            "|pick jonathan tay...|    null|            0.0|\n",
            "|nice local haul n...|    null|            0.3|\n",
            "|                dear|    null|            0.0|\n",
            "|garrett taylor he...|    null|            0.0|\n",
            "+--------------------+--------+---------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro7YRy-7rgeG",
        "colab_type": "text"
      },
      "source": [
        "### 5.2. Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_BTM17ShaBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def condition(r):\n",
        "    if (r >=0.1):\n",
        "        label = \"positive\"\n",
        "    elif(r <= -0.1):\n",
        "        label = \"negative\"\n",
        "    else:\n",
        "        label = \"neutral\"\n",
        "    return label\n",
        "\n",
        "# Setup PySpark UDF Function\n",
        "sentiment_udf = udf(lambda x: condition(x), StringType())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vy2leJ9jhaEE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "413870fc-bea3-44b4-db0c-5d2b7b3c494f"
      },
      "source": [
        "data = data.withColumn(\"sentiment\", sentiment_udf(data['sentiment_score']))\n",
        "data.show(5)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+---------------+---------+\n",
            "|        cleaned_text|hashtags|sentiment_score|sentiment|\n",
            "+--------------------+--------+---------------+---------+\n",
            "|develop code comp...|      []|            0.0|  neutral|\n",
            "|pick jonathan tay...|    null|            0.0|  neutral|\n",
            "|nice local haul n...|    null|            0.3| positive|\n",
            "|                dear|    null|            0.0|  neutral|\n",
            "|garrett taylor he...|    null|            0.0|  neutral|\n",
            "+--------------------+--------+---------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8BVW-a9haGd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "61c58097-a353-4696-8a67-f3095bf58bf5"
      },
      "source": [
        "data.groupBy(\"sentiment\").count().orderBy(\"sentiment\").show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+-----+\n",
            "|sentiment|count|\n",
            "+---------+-----+\n",
            "| negative|  171|\n",
            "|  neutral| 1238|\n",
            "| positive|  748|\n",
            "+---------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GAt0yHLhaI2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5eb07390-6a50-415b-8c47-ee13b21cacbb"
      },
      "source": [
        "# Total cleaned dataset length\n",
        "print(data.count())\n",
        "\n",
        "# Percentages\n",
        "print(171/2157)\n",
        "print(1238/2157)\n",
        "print(748/2157)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2157\n",
            "0.07927677329624479\n",
            "0.573945294390357\n",
            "0.34677793231339826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NGjeJwbr2iJ",
        "colab_type": "text"
      },
      "source": [
        "**Results:** The first impression of these results is that the data cleaning process negated a ton of our initial scraped dataset. This could be due to there being a lot of null values, duplicates, or incorrectly formatted tweet data during the scraping process. The percentage breakdown for each of the sentiment categories within the dataset are as follows:\n",
        "\n",
        "- **Negative: 7.93%**\n",
        "- **Neutral: 57.39%**\n",
        "- **Positive: 34.68%**\n",
        "\n",
        "There's a heavy skew towards neutral sentiment, however, the positive tweets greatly outnumber the negative tweets. It is safe to say that overall, the NFL fanbase's sentiment towards this uncommon draft structure was positive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdYqmk0rr4g6",
        "colab_type": "text"
      },
      "source": [
        "### 5.4. Analysis by Team"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gqd2msP0r5ls",
        "colab_type": "text"
      },
      "source": [
        "While utilizing only the teams hashtags contained in this dataset may not be the most ideal way to view each individual teams fans' sentiment, I'm curious to see the results in ours. I will be viewing sentiment towards the Green Bay Packers, Arizona Cardinals, and Kansas City Chiefs. The Packers had the worst draft in terms of consensus by analysts around the industry, while the Cardinals had one of the better drafts. The Chiefs are the returning superbowl champions, and have a relatively neutral consensus draft grade amongst analysts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtN0ZqPOhaK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Finding distinct values in hashtag column (returns a large amount)\n",
        "# hashtags = [i.hashtags for i in data.select('hashtags').distinct().collect()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5AAC1_Vr8wB",
        "colab_type": "text"
      },
      "source": [
        "#### 5.4.1. Green Bay Packers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m48sHU9Ohgu9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "0f7be587-04a6-4360-cadf-9a10a60bc359"
      },
      "source": [
        "# Packers sentiment\n",
        "data.filter(data.hashtags.contains('Packers')).groupBy(\"sentiment\").count().orderBy(\"sentiment\").show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+-----+\n",
            "|sentiment|count|\n",
            "+---------+-----+\n",
            "| negative|    4|\n",
            "|  neutral|    8|\n",
            "| positive|    4|\n",
            "+---------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOFOHf8dr_LX",
        "colab_type": "text"
      },
      "source": [
        "#### 5.4.2. Arizona Cardinals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aFUmKZlhgxg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "f8f2959c-69f9-4404-918d-562d314f04f6"
      },
      "source": [
        "# Cardinals sentiment\n",
        "data.filter(data.hashtags.contains('Cardinals')).groupBy(\"sentiment\").count().orderBy(\"sentiment\").show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+-----+\n",
            "|sentiment|count|\n",
            "+---------+-----+\n",
            "|  neutral|    7|\n",
            "| positive|    2|\n",
            "+---------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeFx4gk3sA9L",
        "colab_type": "text"
      },
      "source": [
        "#### 5.4.3. Kansas City Chiefs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRd4MvWvhgzr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "3d078e05-005f-46ad-ee8b-e9c180c31deb"
      },
      "source": [
        "# Chiefs sentiment\n",
        "data.filter(data.hashtags.contains('Chiefs')).groupBy(\"sentiment\").count().orderBy(\"sentiment\").show()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+-----+\n",
            "|sentiment|count|\n",
            "+---------+-----+\n",
            "| negative|    2|\n",
            "|  neutral|    4|\n",
            "| positive|    4|\n",
            "+---------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x0yjezvsDHg",
        "colab_type": "text"
      },
      "source": [
        "Again, the analysis suffers from a small sample size. An analysis for each team would likely be stronger if you scraped only keywords or hashtags that are team-specific."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oELTeHgosErf",
        "colab_type": "text"
      },
      "source": [
        "# 6. Naive Bayes Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq8w92yXsGcM",
        "colab_type": "text"
      },
      "source": [
        "Naive Bayes classifiers are a collection of classification algorithms based on **Bayes' Theorem**. This is a family of algorithms where every pair of features being classified is independent of each other. PySpark MLlib supports multinomial naive Bayes and Bernoulli naive Bayes. These models are typically used for document classification.\n",
        "\n",
        "In probability theory and statistics, **Bayes' Theorem** decribes the probability of an event based on prior knowledge of conditions that might be related to the event. Bayes' Theorem is mathematically stated as the equation below. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWhT3IcksIji",
        "colab_type": "text"
      },
      "source": [
        "![](https://drive.google.com/uc?id=1gbNLZ6dwh1zvCMlsjBpjhlf0TWP-U-KB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iclR12AOsPYH",
        "colab_type": "text"
      },
      "source": [
        "Where A and B are events and P(B) ? 0.\n",
        "- Trying to find probability of event A, given the event B is true. Event B is also termed as evidence.\n",
        "- P(A) is the prior probability of A. The evidence is an attribute value of an unknown instance(here, it is event B).\n",
        "- P(A|B) is a probability of event after evidence is seen of B."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeT9yzuUsP4F",
        "colab_type": "text"
      },
      "source": [
        "### 6.1. Create DataFrame for Naive Bayes Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0QHbhImsRdj",
        "colab_type": "text"
      },
      "source": [
        "Due to some Java Heap Memory error complications, I will be performing this on a subset of the overall data (1000 rows)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmAVuGKthg18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweetsubdf = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('NFLDraftTweets_small.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fT5rZh9hg4L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "24a3e9e7-2d43-47d6-f04e-e697c1cf2014"
      },
      "source": [
        "# Create newtweetdf with text and hashtag columns\n",
        "NBdf = tweetsubdf.drop('username', 'acctdesc', 'location', 'following', 'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts', 'retweetcount')\n",
        "\n",
        "# Drop duplicate rows\n",
        "NBdf = NBdf.dropDuplicates()\n",
        "\n",
        "# Drop rows with only null values\n",
        "NBdf = NBdf.dropna(how='all')\n",
        "\n",
        "# Remove non-ASCII characters\n",
        "NBdf = NBdf.withColumn('text_non_asci',strip_non_ascii_udf(NBdf['text']))\n",
        "\n",
        "# Fix abbreviations\n",
        "NBdf = NBdf.withColumn('fixed_abbrev',fix_abbreviation_udf(NBdf['text_non_asci']))\n",
        "\n",
        "#Get raw columns\n",
        "raw_cols = NBdf.columns\n",
        "\n",
        "# Remove stopwords\n",
        "NBdf = NBdf.select(raw_cols).withColumn(\"stopword_text\", remove_stops_udf(NBdf[\"fixed_abbrev\"]))\n",
        "\n",
        "# Remove irrelevant features\n",
        "NBdf = NBdf.select(raw_cols+[\"stopword_text\"]).withColumn(\"rem_feat_text\", remove_features_udf(NBdf[\"stopword_text\"]))\n",
        "\n",
        "# Lemmatization\n",
        "NBdf = NBdf.select(raw_cols+[\"rem_feat_text\"]).withColumn(\"cleaned_text\", lemmatize_udf(NBdf[\"rem_feat_text\"]))\n",
        "\n",
        "# DF for Sentiment analysis\n",
        "NBdf = NBdf.select('cleaned_text','hashtags')\n",
        "NBdf.show(1)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+\n",
            "|        cleaned_text|hashtags|\n",
            "+--------------------+--------+\n",
            "|post nfldraft rav...|    null|\n",
            "+--------------------+--------+\n",
            "only showing top 1 row\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUIWAoARhg6k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "567d8313-5bab-4e35-be83-28daff421420"
      },
      "source": [
        "# Sentiment analysis on 1000 tweet dataset\n",
        "NBdf = NBdf.withColumn(\"sentiment_score\", sentiment_analysis_udf(NBdf['cleaned_text']))\n",
        "NBdf.show(1)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+---------------+\n",
            "|        cleaned_text|hashtags|sentiment_score|\n",
            "+--------------------+--------+---------------+\n",
            "|post nfldraft rav...|    null|            0.0|\n",
            "+--------------------+--------+---------------+\n",
            "only showing top 1 row\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vYFiLKOhg88",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "079c4323-2da6-4f8e-c16a-1ac8c4c84f53"
      },
      "source": [
        "# Create DataFrame for Naive Bayes\n",
        "NBdf = NBdf.selectExpr(\"cleaned_text as text\", \"sentiment_score as label\")\n",
        "NBdf.show(1)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----+\n",
            "|                text|label|\n",
            "+--------------------+-----+\n",
            "|post nfldraft rav...|  0.0|\n",
            "+--------------------+-----+\n",
            "only showing top 1 row\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQGGREzkh75v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e2c33dda-5609-423c-f36c-0c6f987e63ea"
      },
      "source": [
        "# Add unique ID\n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "\n",
        "NBdf = NBdf.withColumn(\"uid\", monotonically_increasing_id())\n",
        "NBdf = NBdf.select('uid', 'text', 'label')\n",
        "NBdf.show(5)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+--------------------+-----+\n",
            "|        uid|                text|label|\n",
            "+-----------+--------------------+-----+\n",
            "| 8589934592|post nfldraft rav...|  0.0|\n",
            "| 8589934593|check ezekiel ell...|  0.0|\n",
            "|17179869184|          tiger king|  0.0|\n",
            "|17179869185|cowboy nfldraft g...|  0.2|\n",
            "|17179869186|want make life ea...|  0.0|\n",
            "+-----------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbVg4rwLsVs_",
        "colab_type": "text"
      },
      "source": [
        "### 6.2. Splitting Data Into Training and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQDpPqUKh-0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the data into training and test sets (70% / 30% split)\n",
        "(trainingData, testData) = NBdf.randomSplit([0.7, 0.3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pBu-uITsY6b",
        "colab_type": "text"
      },
      "source": [
        "### 6.3. Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eek3J4nkh-24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import PySpark Machine Learning Packages\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder\n",
        "from pyspark.ml.tuning import CrossValidator\n",
        "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
        "from pyspark.ml.feature import CountVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uU6WE8Ivh-5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and nb.\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"rawFeatures\")\n",
        "\n",
        "# vectorizer = CountVectorizer(inputCol= \"words\", outputCol=\"rawFeatures\")\n",
        "idf = IDF(minDocFreq=3, inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "\n",
        "# Naive Bayes model\n",
        "nb = NaiveBayes()\n",
        "\n",
        "# Pipeline Architecture\n",
        "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, nb])\n",
        "\n",
        "# Train model.  This also runs the indexers.\n",
        "model = pipeline.fit(trainingData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqdZBKt0sdM2",
        "colab_type": "text"
      },
      "source": [
        "### 6.4. Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BaFhGUvh-7q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2f7433fa-fce9-48d5-f5e9-e7d98ae093cb"
      },
      "source": [
        "# Make predictions\n",
        "predictions = model.transform(testData)\n",
        "\n",
        "# Select example rows to display.\n",
        "predictions.select(\"text\", \"label\", \"prediction\").show(5)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+------+----------+\n",
            "|                text| label|prediction|\n",
            "+--------------------+------+----------+\n",
            "|cowboy nfldraft g...|   0.2|       8.0|\n",
            "|want make life ea...|   0.0|      16.0|\n",
            "|safety get draft ...|  -0.2|       8.0|\n",
            "|tradition unlike ...|-0.125|       8.0|\n",
            "|derrick brown hea...|   0.0|       8.0|\n",
            "+--------------------+------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BM_P0Zesf0K",
        "colab_type": "text"
      },
      "source": [
        "### 6.5. Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6c_vGr_h--B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "793eeaac-98ce-4af2-8e11-c4ef7c8d9540"
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator.evaluate(predictions)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tDcXEWysldT",
        "colab_type": "text"
      },
      "source": [
        "The Naive Bayes model strangely returned an accuracy of 0%. It appears as though it is trying to make predictions outside of the contraints that a sentiment analysis can be. Let's try a different model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAhiDWV9strY",
        "colab_type": "text"
      },
      "source": [
        "# 7. CountVectorizer + IDF + Logistic Regression Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Qc8qFHGt6ti",
        "colab_type": "text"
      },
      "source": [
        "Here, I will try to implement a model that gathers term frequency for IDF (Inverse Document Frequency) calculation. \n",
        "\n",
        "* An inverse document frequency factor is incorporated to diminish the weight of terms that occur very frequently in document sets and increase the weight of terms that occur rarely.\n",
        "\n",
        "* CountVectorizer and CountVectorizerModel in PySpark.ml aim to help convert a collection of text documents to vectors of token counts.\n",
        "\n",
        "* Logistic regression models are used to model probabilities of a certain class or event such as pass/fail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC22DgmMumK-",
        "colab_type": "text"
      },
      "source": [
        "### 7.1. Creating DataFrame for Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXjlvvJpjgb3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "e9bab251-9dd2-4ab6-effd-2912e75a08c5"
      },
      "source": [
        "modelData = NBdf.selectExpr(\"text as text\", \"label as target\")\n",
        "modelData.show(1)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+------+\n",
            "|                text|target|\n",
            "+--------------------+------+\n",
            "|post nfldraft rav...|   0.0|\n",
            "+--------------------+------+\n",
            "only showing top 1 row\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlbiT5O8jgef",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "3b451cfb-df03-437b-acf2-cdaa37b325ea"
      },
      "source": [
        "# Add unique ID\n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "\n",
        "modelData = modelData.withColumn(\"uid\", monotonically_increasing_id())\n",
        "modelData = modelData.select('uid', 'text', 'target')\n",
        "modelData.show(1)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+--------------------+------+\n",
            "|       uid|                text|target|\n",
            "+----------+--------------------+------+\n",
            "|8589934592|post nfldraft rav...|   0.0|\n",
            "+----------+--------------------+------+\n",
            "only showing top 1 row\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYI8t8HwupLG",
        "colab_type": "text"
      },
      "source": [
        "### 7.2. Splitting Data Into Training and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvKBWGvejggU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train_set, val_set, test_set) = modelData.randomSplit([0.98, 0.01, 0.01], seed = 2000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wFXfe7Quqt6",
        "colab_type": "text"
      },
      "source": [
        "### 7.3. Modeling and Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIq3gBqUh_DH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "78cfe8be-b926-49be-865f-8d0047b0c4f2"
      },
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n",
        "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
        "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
        "lr = LogisticRegression(maxIter=100)\n",
        "pipeline = Pipeline(stages=[tokenizer, cv, idf, label_stringIdx, lr])\n",
        "\n",
        "pipelineFit = pipeline.fit(train_set)\n",
        "predictions = pipelineFit.transform(val_set)\n",
        "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
        "roc_auc = evaluator.evaluate(predictions)\n",
        "\n",
        "accuracy\n",
        "roc_auc"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5333333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQKk83bhu7bF",
        "colab_type": "text"
      },
      "source": [
        "The CountVectorizer + IDF + Logistic Regression Model returns a better accuracy, albeit still rather low."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzVR6m1cu2Ls",
        "colab_type": "text"
      },
      "source": [
        "# 8. References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2I4RH-Su2OR",
        "colab_type": "text"
      },
      "source": [
        "**Content Sources:**\n",
        "- https://www.techopedia.com/definition/29019/cloud-server\n",
        "- https://www.pythonanywhere.com\n",
        "- http://docs.tweepy.org/en/latest/\n",
        "- https://www.geeksforgeeks.org/twitter-sentiment-analysis-using-python/\n",
        "- http://adilmoujahid.com/posts/2014/07/twitter-analytics/\n",
        "- https://stackoverflow.com/questions/24214189/how-can-i-get-tweets-older-than-a-week-using-tweepy-or-other-python-libraries\n",
        "- http://spark.apache.org/docs/latest/#launching-on-a-cluster\n",
        "- https://docs.databricks.com/data/data-sources/read-csv.html\n",
        "- https://towardsdatascience.com/creating-the-twitter-sentiment-analysis-program-in-python-with-naive-bayes-classification-672e5589a7ed\n",
        "- https://towardsdatascience.com/sentiment-analysis-with-pyspark-bc8e83f80c35\n",
        "- https://medium.com/@leowgriffin/scraping-tweets-with-tweepy-python-59413046e788\n",
        "- https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html\n",
        "- https://runawayhorse001.github.io/LearningApacheSpark/textmining.html\n",
        "- https://www.nltk.org/\n",
        "- https://classes.ischool.syr.edu/ist718/content/unit09/lab-sentiment_analysis/\n",
        "- http://www.datasciencemadesimple.com/subset-or-filter-data-with-multiple-conditions-in-pyspark/\n",
        "- https://spark.apache.org/docs/2.4.5/\n",
        "- http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.NaiveBayes\n",
        "- Prior notebooks created for homeworks, lecture notes, other classes' materials\n",
        "\n",
        "**Photo sources:**\n",
        "- http://spark.apache.org/\n",
        "- https://realpython.com/twitter-bot-python-tweepy/\n",
        "- https://oh42fifty.org/wp-content/uploads/2020/05/Winners_and_Losers_of_the_2020_NFL_Draft.jpg\n",
        "- https://runawayhorse001.github.io/LearningApacheSpark/_images/sentiment_analysis_pipeline.png\n",
        "- https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-7777aa719ea14857115695676adc0914_l3.svg"
      ]
    }
  ]
}